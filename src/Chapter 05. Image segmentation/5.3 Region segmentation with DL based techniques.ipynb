{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3 Region segmentation with DL based techniques\n",
        "\n",
        "Deep Learning (DL) based techniques for region segmentation follow the typical pipeline of this Artificial Intelligence branch:\n",
        "- A huge dataset with segmented regions is collected.\n",
        "- A model arquitecture is designed.\n",
        "- The model is trained with the dataset until its performance is over the application requirements.\n",
        "- It is released/deployed.\n",
        "\n",
        "In this notebook we will explore Segment Anything Model (SAM)."
      ],
      "metadata": {
        "id": "OYYTzgpZrckH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem context - Region segmentation to calculate vegetation indices\n",
        "\n",
        "In our country, climate change is causing serious problems for green areas, as periods of drought are becoming more frequent due to a lack of rain. This situation compromises the irrigation of gardens and crops, often leading to water supply cuts or the use of reclaimed water, which is not always ideal for plants.\n",
        "\n",
        "Precision agriculture offers a way to mitigate this issue through the monitoring of green areas and the extraction of indices that can help estimate the plants' condition. These indices can measure factors like water stress (lack of water), the presence of disease, vegetative vigor, etc. With this information, we can provide tailored treatment to plants based on their specific needs, optimizing water usage and reducing waste.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/jotaraul/cv_hub/refs/heads/main/segmentation/images_crops/expected_segmentation_and_ndvi_result_2.PNG\" width=\"600\"></center>\n",
        "\n",
        "Computer vision to the rescue! In this practical exercise, our task is to segment trees visible in an image of a crop field captured by a drone. By doing so, we can extract personalized data for each tree, allowing us to provide the exact amount of water each one needs. This exercise introduces an essential application of computer vision in environmental sustainability and precision farming.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IUtnq9PzmBPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing our setup"
      ],
      "metadata": {
        "id": "3WZUhIVAyFzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To work with SAM you need to request GPUs power to Google colab. To do that click on \"Entorno de ejecución\" > \"Cambiar tipo de entorno de ejecución\".\n",
        "Choose, for example, T4 GPU.\n",
        "\n",
        "Let's start by installing the required python packages to work with SAM:"
      ],
      "metadata": {
        "id": "keRER1lHxSHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy6Pi2nwE6lg"
      },
      "outputs": [],
      "source": [
        "!pip3 install segment_anything\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth # Download model weights\n",
        "# Model checkpoints\n",
        "# vit_h, https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "# vit_l, https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
        "# vit_b, https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RW1KdInFG_M"
      },
      "source": [
        "Now let's import the required libraries and modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8cu-aI3EQpY"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import requests # to load content from the internet\n",
        "from numba import cuda\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now explore the images we'll be working with. These include:\n",
        "\n",
        "- **RGB Image:** This will be used for segmenting the visible regions (trees).\n",
        "- **Near-Infrared (NIR) and Red Images:** These are essential for calculating vegetation indices, such as the NDVI, which help assess plant health.\n",
        "\n",
        "All of these images were captured by a multispectral camera mounted on a drone during a flight over a crop field.\n",
        "\n",
        "*Hint: if you try your own pictures, take care with their size, you can run out of memory. For example, in Colab that happens with images of size (3956, 5280, 3).*"
      ],
      "metadata": {
        "id": "ojH1HrmntDDo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74xnKsb2Fe2m"
      },
      "outputs": [],
      "source": [
        "def load_image_from_url( url, image_type ):\n",
        "\n",
        "  response = requests.get(url)\n",
        "  image_data = np.frombuffer(response.content, np.uint8)  # Convert to a numpy array\n",
        "  image = cv2.imdecode(image_data, image_type )\n",
        "  return image\n",
        "\n",
        "images_path = 'https://github.com/jotaraul/cv_hub/blob/main/segmentation/images_crops/'\n",
        "\n",
        "image_RGB = load_image_from_url( images_path + '1_RGB.JPG?raw=true', cv2.IMREAD_COLOR)\n",
        "image_NIR = load_image_from_url( images_path + '1_NIR.TIF?raw=true', cv2.IMREAD_GRAYSCALE)\n",
        "image_R = load_image_from_url( images_path + '1_R.TIF?raw=true', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# You can also load images from a local path / Google Drive, if you have\n",
        "# When working with Google Colab, you have to provide access to your Google\n",
        "# Drive so it can access your files, e.g., your images.\n",
        "# This is done using the mount() function.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "# images_path = '/gdrive/MyDrive/Colab Notebooks/Images Riego/'\n",
        "# image_RGB = cv2.imread(images_path + '1_RGB.JPG')\n",
        "# image_NIR = cv2.imread(images_path + '1_NIR.TIF', cv2.IMREAD_UNCHANGED)\n",
        "# image_R=cv2.imread(images_path + '1_R.TIF',cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "print('[Image info]')\n",
        "\n",
        "#print('Data type:',image.dtype)\n",
        "print('RGB_Data size:', image_RGB.shape)\n",
        "print('NIR_Data size:', image_NIR.shape)\n",
        "print('Red_Data size:', image_R.shape)\n",
        "\n",
        "# Resize RGB image to share dimensions with NIR and Red images\n",
        "nir_height, nir_width = image_NIR.shape[:2]\n",
        "image_RGB = cv2.resize(image_RGB, (nir_width, nir_height))\n",
        "print('RGB_Data resized size:', image_RGB.shape)\n",
        "\n",
        "image_RGB = cv2.cvtColor(image_RGB, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "\n",
        "plt.subplot(2,2,1), plt.axis('off')\n",
        "plt.title('RGB image')\n",
        "plt.imshow(image_RGB)\n",
        "\n",
        "plt.subplot(2,2,2), plt.axis('off')\n",
        "plt.title('NIR image')\n",
        "plt.imshow(image_NIR, cmap='inferno') # 'inferno' is good for high contrast in intensity\n",
        "\n",
        "plt.subplot(2,2,3), plt.axis('off')\n",
        "plt.title('Red image')\n",
        "plt.imshow(image_R, cmap='inferno');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment Anything Model (SAM)\n",
        "\n",
        "SAM is a large language model developed by **Meta AI** (formerly Facebook AI Research). It was trained on the extensive SA-1B dataset, which contains 1.1 billion segmentation masks, enabling the model to generalize effectively to new, unseen data due to its exposure to a highly diverse dataset.\n",
        "\n",
        "SAM can be used to generate masks for any image without needing labeled data. This marks a major breakthrough, as fully automated segmentation wasn't achievable before SAM.\n",
        "\n",
        "What sets SAM apart is its role as the first **promptable segmentation** model. Users can guide the model's output using various prompts, including points, bounding boxes, text instructions, and even base masks."
      ],
      "metadata": {
        "id": "qUxzVunEyfNC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmGDeKyEGLmK"
      },
      "source": [
        "Auxiliar function to show the SAM results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6sRRRJQFbJh"
      },
      "source": [
        "## Choose a SAM model to work with.\n",
        "\n",
        "You can choose from three options of checkpoint weights: ViT-B (91M), ViT-L (308M), and ViT-H (636M parameters). How do you choose the right one? The larger the number of parameters, the longer the time needed for inference, that is mask generation. If you have low GPU resources and fast inference, go for ViT-B. Otherwise, choose ViT-H.\n",
        "\n",
        "In this notebook we previously downloaded the ViT-H checkpoint weights, but you can try the other ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)"
      ],
      "metadata": {
        "id": "466hmKp63IeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segmenting every region in an image\n",
        "\n",
        "Since SAM can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image. This method was used to generate the dataset SA-1B.\n",
        "\n",
        "The class `SamAutomaticMaskGenerator` implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes.\n",
        "\n",
        "There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:\n",
        "\n",
        "```\n",
        "mask_generator_2 = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,\n",
        "    pred_iou_thresh=0.86,\n",
        "    stability_score_thresh=0.92,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2,\n",
        "    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "noAlzjvZZLCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For completeness, this is the full list of parameters, but you wouldn't need to modify most of them.\n",
        "\n",
        "- `model` (Sam, default SAM): The SAM model to use for mask prediction.\n",
        "\n",
        "- `points_per_side` (int or None, default 32): The number of points to be sampled along one side of the image. The total number of points is\n",
        "`points_per_side**2`. If None, 'point_grids' must provide explicit point sampling.\n",
        "\n",
        "- `points_per_batch` (int, default 64): Sets the number of points run simultaneously by the model. Higher numbers may be faster but use more GPU memory.\n",
        "\n",
        "- `pred_iou_thresh` (float, default 0.88): A filtering threshold in [0,1], using the model's predicted mask quality.\n",
        "\n",
        "- `stability_score_thresh` (float, default 0.95): A filtering threshold in [0,1], using the stability of the mask under changes to the cutoff used to binarize the model's mask predictions.\n",
        "\n",
        "- `stability_score_offset` (float, default 1.0): The amount to shift the cutoff when calculated the stability score.\n",
        "          \n",
        "- `box_nms_thresh` (float, default 0.7): The box IoU cutoff used by non-maximal suppression to filter duplicate masks.\n",
        "\n",
        "- `crop_n_layers` (int, default 0): If >0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where each layer has 2**i_layer number of image crops.\n",
        "\n",
        "- `crop_nms_thresh` (float, 0.7): The box IoU cutoff used by non-maximal suppression to filter duplicate masks between different crops.\n",
        "          \n",
        "- `crop_overlap_ratio` (float, default 512/1500): Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of the image length. Later layers with more crops scale down this overlap.\n",
        "\n",
        "- `crop_n_points_downscale_factor` (int, default 1): The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n",
        "\n",
        "- `point_grids` (list(np.ndarray) or None, default None): A list over explicit grids of points used for sampling, normalized to [0,1]. The nth grid in the list is used in the nth crop layer. Exclusive with points_per_side.\n",
        "\n",
        "- `min_mask_region_area` (int, default 0): If >0, postprocessing will be applied to remove disconnected regions and holes in masks with area smaller than min_mask_region_area. Requires opencv.\n",
        "\n",
        "- `output_mode` (str, default \"binary_mask\"): The form masks are returned in. Can be 'binary_mask', 'uncompressed_rle', or 'coco_rle'. 'coco_rle' requires pycocotools. For large resolutions, 'binary_mask' may consume large amounts of memory."
      ],
      "metadata": {
        "id": "cTnttCNCGy8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool! Let's create the automatic mask generator object:"
      ],
      "metadata": {
        "id": "ekutJjv23bP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww4Gf6w5Ee4o"
      },
      "outputs": [],
      "source": [
        "sam.cuda()\n",
        "mask_generator = SamAutomaticMaskGenerator(sam,\n",
        "                                           stability_score_thresh=0.96,\n",
        "                                           min_mask_region_area=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DmEX9FhFebh"
      },
      "source": [
        "`SamAutomaticMaskGenerator` returns a list of masks, where each mask is a dict containing various information about the mask:\n",
        "\n",
        "- `segmentation` - [np.ndarray] - the mask with (W, H) shape, and bool type.\n",
        "- `area` - [int] - the area of the mask in pixels\n",
        "- `bbox` - [List[int]] - the boundary box of the mask in xywh format\n",
        "- `predicted_iou` - [float] - the model's own prediction for the quality of the mask\n",
        "- `point_coords` - [List[List[float]]] - the sampled input point that generated this mask\n",
        "- `stability_score` - [float] - an additional measure of mask quality\n",
        "- `crop_box` - List[int] - the crop of the image used to generate this mask in xywh format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70C7zMW4EP7t"
      },
      "source": [
        "## *ASSIGNMENT 1: Region segmentation with SAM, how the output looks like?*\n",
        "\n",
        "Our first task is to run our `mask_generator`to retrieve the masks that SAM segments in the RGB image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the time taken for mask generation\n",
        "start_time = time.time()  # Start timing\n",
        "\n",
        "# Generate masks\n",
        "masks = mask_generator.generate(None)\n",
        "\n",
        "end_time = time.time()  # End timing\n",
        "time_taken = end_time - start_time\n",
        "\n",
        "print('[Results info]')\n",
        "\n",
        "print(f\"Time taken to generate masks: {None:.2f} seconds\")"
      ],
      "metadata": {
        "id": "Y4MD4DGSq3C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in the next code cell, implement the needed code to print the total number of masks detected by SAM, and the content of the first mask (segmentation, area, etc.)."
      ],
      "metadata": {
        "id": "do8c5bEe5BVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('[Results info]')\n",
        "\n",
        "print('Number of masks:', None)\n",
        "\n",
        "print('Mask[0] data:')\n",
        "for None,None in None:\n",
        "  print (None,None)"
      ],
      "metadata": {
        "id": "LBP2G7D35lGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output similar to:\n",
        "\n",
        "```\n",
        "[Results info]\n",
        "Number of masks: 101\n",
        "Mask[0] data:\n",
        "segmentation [[False False False ... False False False]\n",
        " [False False False ... False False False]\n",
        " [False False False ... False False False]\n",
        " ...\n",
        " [False False False ... False False False]\n",
        " [False False False ... False False False]\n",
        " [False False False ... False False False]]\n",
        "area 79361\n",
        "bbox [795, 32, 349, 394]\n",
        "predicted_iou 1.0044721364974976\n",
        "point_coords [[850.5, 151.875]]\n",
        "stability_score 0.9818553924560547\n",
        "crop_box [0, 0, 2592, 1944]\n",
        "```"
      ],
      "metadata": {
        "id": "dKYd4nQl6_uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *ASSIGNMENT 2: Computing NDVI values of vegetation*\n",
        "\n",
        "The **Normalized Difference Vegetation Index (NDVI)** is a popular index used in remote sensing to assess vegetation health and density. It measures the difference between the Near-Infrared (NIR) and Red (R) spectral bands, which are sensitive to vegetation:\n",
        "\n",
        "- NIR: Healthy vegetation reflects strongly in the Near-Infrared band.\n",
        "- Red: Vegetation absorbs light in the Red band for photosynthesis, leading to low reflection.\n",
        "\n",
        "The NDVI is calculated using the formula:\n",
        "\n",
        "$$\n",
        "NDVI = \\frac{(NIR+Red)}{(NIR−Red)}\n",
        "​$$\n",
        "\n",
        "Values range from -1 to 1:\n",
        "- Close to 1: Healthy, dense vegetation.\n",
        "- Close to 0: Bare soil or sparse vegetation.\n",
        "- Negative values: Water, clouds, or non-vegetative features.\n",
        "\n",
        "Using the multispectral images provided (NIR and Red), your task is to compute the NDVI values of the entire image and show them."
      ],
      "metadata": {
        "id": "JejUin398Z72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the bands to float for NDVI calculation\n",
        "nir_band = image_NIR.astype(float)\n",
        "red_band = image_R.astype(float)\n",
        "\n",
        "# Compute NDVI: (NIR - Red) / (NIR + Red)\n",
        "ndvi = None\n",
        "\n",
        "# Display the NDVI image using a color map\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(None, cmap=\"RdYlGn\")  # RdYlGn for a green-to-red color map (green = healthy vegetation)\n",
        "plt.colorbar(label='NDVI')\n",
        "plt.title(\"NDVI Map\")\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "3kkMrNrLSqzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/jotaraul/cv_hub/refs/heads/main/segmentation/images_crops/expected_ndvi_computation_result_1.PNG\" width=\"400\">"
      ],
      "metadata": {
        "id": "fWgmn8ac-41C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal now will be to compute the mean and standard deviation of the ndvi values of the segmented regions. But first, let's start just by using the `np.ma.masked_where()` function along with the segmentation mask of the first detected one (`masks[0]`) and the computed ndvi values to produce a masked_image where just `true` values in the segmentation mask take their ndvi values."
      ],
      "metadata": {
        "id": "pEBxP_WUHlpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the masked image\n",
        "masked_image = np.ma.masked_where(None, None)\n",
        "\n",
        "# Show it!\n",
        "plt.imshow(None)\n",
        "plt.title('Pixels belonging to the first segmented region (masks[0]) \\n and their ndvi values');"
      ],
      "metadata": {
        "id": "lie9xNXKIbz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/jotaraul/cv_hub/refs/heads/main/segmentation/images_crops/expected_masked_image_result_1.PNG\" width=\"400\">"
      ],
      "metadata": {
        "id": "FodeVx0aKKte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you know how to generate an image with just the ndvi values of a segmented region, let's compute the mean and standard deviation of those values per region. Then, they are included in the dictionary of each mask."
      ],
      "metadata": {
        "id": "sC-CWPRlI_0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-NrlCZlb_01"
      },
      "outputs": [],
      "source": [
        "# Iterate over the masks\n",
        "for mask in masks:\n",
        "  # Get the masked image\n",
        "  masked_image = np.ma.masked_where(None, None)\n",
        "  # Compure and store the mean\n",
        "  mask['nvdi_mean'] = None\n",
        "  # Compute and store the standard deviation\n",
        "  mask['nvdi_stdv'] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *ASSIGNMENT 3: Visualizing the results*\n",
        "\n",
        "Wow! We did a great job segmenting the regions appearing in the RGB image, computing the ndvi values using the NIR and Red ones, and retrieving their mean and standard deviation values of each mask. But something is missing... it would be cool to visualize the result of all this effort, isn't it?\n",
        "\n",
        "Your task is to complete the following function that, over the initial RGB image, shows the $n$ biggest masks, and uses their mean ndvi value to draw in the middle of their bounding boxes a circle which color means:\n",
        "\n",
        "- Green: healthy vegetation.\n",
        "- Orange: Bare soil or sparse vegetation.\n",
        "- Red:  Water, clouds, or non-vegetative features."
      ],
      "metadata": {
        "id": "po-mzmyzK9q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_segmented_regions(image, masks, show_n_biggest=0):\n",
        "\n",
        "    if len(masks) == 0:\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 9))\n",
        "\n",
        "    sorted_masks = sorted(masks, key=(lambda x: x['area']), reverse=True)\n",
        "\n",
        "    if show_n_biggest:\n",
        "      sorted_masks = sorted_masks[None]\n",
        "\n",
        "    # Let's create an image with transparency, being the fourth channel said\n",
        "    # transparency\n",
        "    image_masks = np.ones((sorted_masks[0]['segmentation'].shape[0],\n",
        "                          sorted_masks[0]['segmentation'].shape[1],\n",
        "                          4))\n",
        "    image_masks[:,:,3] = 0\n",
        "\n",
        "    # Only show nvdi_means if they have been computed\n",
        "    if 'nvdi_mean' in sorted_masks[0]:\n",
        "      image_circles = np.ones_like(image_masks, dtype=float)\n",
        "      image_circles[:,:,3] = 0\n",
        "\n",
        "    for mask in sorted_masks:\n",
        "\n",
        "        m = mask['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
        "        image_masks[m] = None\n",
        "\n",
        "        if 'nvdi_mean' in mask:\n",
        "\n",
        "          bbox = mask['bbox']\n",
        "\n",
        "          if mask['nvdi_mean'] > 0.3:\n",
        "            cv2.circle(None,(int(bbox[0]+bbox[2]*0.5),int(bbox[1]+bbox[3]*0.5)), 25, (0,1,0,0.9), -1)\n",
        "\n",
        "          elif mask['nvdi_mean'] >0:\n",
        "            cv2.circle(None,(int(bbox[0]+bbox[2]*0.5),int(bbox[1]+bbox[3]*0.5)), 25, (1,0.5,0,0.9), -1)\n",
        "\n",
        "          else:\n",
        "            cv2.circle(None,(int(bbox[0]+bbox[2]*0.5),int(bbox[1]+bbox[3]*0.5)), 25, (1,0,0.9), -1)\n",
        "\n",
        "    plt.imshow(None)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "    ax.imshow(None)\n",
        "\n",
        "    if 'nvdi_mean' in sorted_masks[0]:\n",
        "      ax.imshow(None)"
      ],
      "metadata": {
        "id": "wfnC13dPaMB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_n_biggest = 10\n",
        "show_segmented_regions(image_RGB, masks, show_n_biggest)"
      ],
      "metadata": {
        "id": "YKxyHmRmbzz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected result (showing the 10 biggest masks):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/jotaraul/cv_hub/refs/heads/main/segmentation/images_crops/expected_segmentation_and_ndvi_result_1.PNG\" width=\"400\">"
      ],
      "metadata": {
        "id": "EgK2mKuWORgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations on completing this notebook! 🎉\n",
        "\n",
        "In this notebook, we explored the power of **Deep Learning (DL)** techniques for **region segmentation** using the cutting-edge **Segment Anything Model (SAM)**. We went through the process of:\n",
        "\n",
        "1. **Loading and Visualizing Multispectral Data**: We worked with **RGB**, **Near-Infrared (NIR)**, and **Red** images captured by a multispectral camera mounted on a drone, which provided us with the necessary data to monitor vegetation.\n",
        "   \n",
        "2. **Segmenting Regions with SAM**: You successfully used the **Segment Anything Model (SAM)** to segment individual trees from the RGB image, demonstrating SAM's ability to generalize effectively without requiring labeled data.\n",
        "\n",
        "3. **Computing NDVI (Normalized Difference Vegetation Index)**: Using the NIR and Red bands, you computed the **NDVI**, a crucial index in precision agriculture that helps monitor the health and vigor of vegetation. We applied NDVI on the segmented trees, which allowed for personalized monitoring of each tree's health based on its spectral properties.\n",
        "\n",
        "4. **Visualizing Results**: The final visualization included not only the segmented regions but also overlaid **NDVI values** represented by color-coded circles, allowing for an easy assessment of vegetation health across the entire image.\n",
        "\n",
        "Through this notebook, you've gained hands-on experience with **multispectral image analysis**, **region segmentation using SAM**, and the **computation and interpretation of NDVI**, which are relevant tools in the fields of **remote sensing** and **precision agriculture**.\n",
        "\n",
        "Well done! You've taken important steps toward understanding how computer vision can be applied to solve real-world problems in environmental monitoring and sustainability. 🌱\n"
      ],
      "metadata": {
        "id": "1xqbB-HaPsP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook was designed using the following sources of information:\n",
        "- SAM paper: https://arxiv.org/abs/2304.02643\n",
        "- https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\n",
        "- https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py"
      ],
      "metadata": {
        "id": "umq6q0PQZhAo"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}