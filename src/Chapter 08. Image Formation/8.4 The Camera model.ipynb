{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XOGAsy3Jcc5s"
   },
   "source": [
    "# 8.4 The Camera model\n",
    "\n",
    "Until now we have first focused in stating the mathematical tools required to understand general vector/coordinates transformations and then to apply them to perform different image transformations (homographies). In fact, remember that in previous notebooks, we have transformed `WORLD` coordinates to `CAMERA` coordinates and the other way around, but that is just a **3D to 3D** transformation. However, we have not properly addressed yet the process of transforming **`WORLD` coordinates (3D)** to **`IMAGE` coordinates (2D)** (and vice versa when possible), that is, how images are formed from real 3D objects. Well, this is the time, let's go for it!\n",
    "\n",
    "In this notebook we will learn:\n",
    "\n",
    "- the **big picture** of the problem that we are addressing (<a href=\"#841\">section 8.4.1</a>), \n",
    "- the **Pinhole model** (<a href=\"#842\">section 8.4.2</a>), and\n",
    "- the **Camera model** (<a href=\"#843\">section 8.4.3</a>).\n",
    "\n",
    "Finally, we will put the learned concepts to work with a practical example (<a href=\"#844\">section 8.4.4</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBfw-zo_cc5t"
   },
   "source": [
    "## Problem context - RGB-D images\n",
    "\n",
    "RGB-depth (RGB-D) images are just like standard RGB images (where each pixel has information about each basic color: red, green and blue) but adding another information to each pixel: **the range/depth from the camera to the 3D point that is projected on it**. That is, these images contain not only photometric information but also geometric information.$\\\\[10pt]$\n",
    "\n",
    "<center><img src=\"./images/kinect.png\" width=\"600\"/>$\\\\[5pt]$</center>\n",
    "\n",
    "In this figure, the left image is the standard RGB image, while the right one is the *depth* band shown as a greyscale image (darker as the object is closer).\n",
    "\n",
    "Although this kind of images have been used for some decades now, in recent years they have become popular due to the development of inexpensive sensors, such as the [Microsoft's Kinect](https://en.wikipedia.org/wiki/Kinect) camera, that are able to directly provide them. In the entertainment bussiness, these images have been used for segmenting the people using the system from the background of the image and inspecting their movements.\n",
    "\n",
    "Roughly speaking, the working principle of these devices (called structured light) involves projecting an infrared pattern on the scene (see image below) and inspecting the deformations that such pattern undergoes due to the irregular surfaces where the light bounces. These deformations are directly related to the distance and position of the object.\n",
    "\n",
    "<center><img src=\"./images/kinect_pattern.png\" width=\"400\"/>$\\\\[5pt]$</center>\n",
    "\n",
    "As RGB-D images actually provide depth information, they are a **good example for learning how image formation models work**, because we can turn images to 3D maps (using the depth) and the other way around (3D scene to 2D image plane)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYZZZ5pUcc5v"
   },
   "source": [
    "## 8.4.1 The big picture <a id=841></a>\n",
    "\n",
    "The process of converting **world coordinates** to **image coordinates** involves several steps that are applied by means of the concatenation of transformations expressed with homogeneous matrices/vectors. We address all these transformations in this notebook but, first, let's present the whole process in a single image, just to keep an eye on **the big picture** and then we will unravel its mysteries.\n",
    "\n",
    "<center><figure>\n",
    "    <img src=\"images/cameramodel.png\" >$\\\\[5pt]$\n",
    "    <figcaption>Fig. 1 - The big picture of the transformation involved in the conversion of a 3D point $M$ from its world coordinates to the image ones $m$.</figcaption>\n",
    "</figure></center>\n",
    "\n",
    "Let's do this step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCvqS0r4cc5w"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.stats as stats\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from math import sin, cos, radians, floor\n",
    "images_path = './images/'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 12.0)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.plot3DScene import plot3DScene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TXlh1TzXcc57"
   },
   "source": [
    "### Homogeneous coordinates (again)\n",
    "\n",
    "As we already know, an interesting property of them is that the **homogeneous coordinates of a point in the plane ($\\mathbb{R}^2$) also represents a line passing through the origin in a reference frame parallel to the image plane**:$\\\\[10pt]$\n",
    "\n",
    "<center><img src=\"./images/homogenous.png\" width=\"400\"/>$\\\\[5pt]$</center>\n",
    "\n",
    "That is, for a certain Cartesian point $\\mathbf{p}=(x,y)$, its homogeneous version $\\tilde{\\mathbf{p}}=(kx,ky,k),\\forall k$ represents a line in 3D starting at the origin of coordinates. This is called a **projection line**. \n",
    " \n",
    "The **projective plane**, called $\\mathbb{P}^2$, is the set of 3-tuples of real numbers such that $\\begin{bmatrix}x_1 \\\\ x_2 \\\\ 1\\end{bmatrix} \\equiv k \\begin{bmatrix}x_1 \\\\ x_2 \\\\ 1\\end{bmatrix}, \\ k \\ne 0$ (equivalent 3-tuples since they represent the same point in $\\mathbb{R}^2$), that is the set of all projective lines at a certain distance $k$ from the origin of coordinates.\n",
    "\n",
    "So, in summary:\n",
    "- A point in $\\mathbb{P}^2$ (3-tuple) is represented in ($\\mathbb{R}^3$) as a line passing through the origin.\n",
    "- The component $k$ can be understood as the *depth*, as it indicates a specific point along the line.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D0Qtw0Pcc57"
   },
   "source": [
    "## 8.4.2 The Pinhole model <a id=842></a>\n",
    "\n",
    "The Pinhole camera model is the simplest model we can think to understand how image formation works [[1]](https://web.stanford.edu/class/cs231a/course_notes/01-camera-models.pdf). Letâ€™s image that we want to design a simple camera system that can record an image of an object in the 3D world. This camera system can be designed by placing a solid barrier with a small  (pin-size) aperture between the 3D object and a photographic film or sensor. \n",
    "\n",
    "<center><img src=\"./images/pinhole_intro.png\" />$\\\\[5pt]$</center>\n",
    "\n",
    "As this figure shows, each point on the 3D object bounces the light from the source and emits multiple rays of such light outwards. Without a barrier in place, every point on the film will be influenced by light rays emitted from every point on the 3D object. However, due to the barrier, only one (or a few) of these rays of light passes through the aperture and hits the film. Therefore, we can establish a one-to-one mapping between points on the 3D object and the film. The result is that the film gets exposed by an *image* of the 3D object by means of this mapping. This simple model is known as **the pinhole camera model**.\n",
    "\n",
    "In the Pinhole model we want to project the 3D world (a set of $\\mathbf{p}_i=[X_i,Y_i,Z_i]^T$ points) in a plane called **image plane**. For that, we have a camera placed at $\\mathbf{C} = [0,0,0]^\\texttt{T}$ in the `WORLD` reference system (i.e. both the `WORLD` and the `CAMERA` systems are coincident). This camera has a fixed property $f$ called **focal length**, which indicates the distance between the the optical center and the **camera sensor** (placed in the interior of the camera). The camera sensor is the plane **where the scene of the real world is projected**:\n",
    "\n",
    "<center><img src=\"./images/pinhole.png\" />$\\\\[5pt]$</center>\n",
    "\n",
    "To operate with this image formation model, we need to know two processes: \n",
    "- to project a 3D world point to the image plane \n",
    "- to back-project a 2D point on the image plane to the 3D world.\n",
    "\n",
    "For this, we are going to use the previously mentioned property of homogenous coordinates. Let's take a look at these processes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D0Qtw0Pcc57"
   },
   "source": [
    "### From 2D to 3D\n",
    "\n",
    "Given a point $\\mathbf{p} = [x,y]^\\texttt{T} \\in \\mathbb{R}^2$ in the image, its **projection line** (in red in the figure above) in the camera system is very simple to compute by means of the line passing through the point $[x,y,f]^\\texttt{T} \\in \\mathbb{R}^3$ (we have added the third component $f$ because the image plane is placed at a distance $f$ in the $Z$-axis of the camera reference system, recall that this is the **focal length**!). In homogeneous coordinates, this line follows the expression:$\\\\[5pt]$\n",
    "\n",
    "$$k \\begin{bmatrix}x \\\\ y \\\\ f \\end{bmatrix} \\in \\mathbb{P}^2$$\n",
    "\n",
    "where $k$ indicates a specific 3D point along the projection line, and $P$ is the so called **projective plane**.\n",
    "\n",
    "For instance, if we set $k=2$ we have the point:$\\\\[5pt]$\n",
    "\n",
    "$$ \\begin{bmatrix}2x \\\\ 2y \\\\ 2f \\end{bmatrix} \\in \\mathbb{R}^3 \\\\[5pt]$$\n",
    "\n",
    "Note that, in this expression, $k$ does not correspond directly to the depth of the 3D point, but we can fix this dividing by the focal length:$\\\\[5pt]$\n",
    "\n",
    "$$\\underbrace{kf}_{Z} \\begin{bmatrix} x/f  \\\\ y/f \\\\ 1 \\end{bmatrix} \\in \\mathbb{P}^2 \\\\[5pt]$$ \n",
    "\n",
    "Now that the third coordinate is 1, $k'=kf$ indicates the depth of the 3D point (that is, it's coordinate along the $Z$ axis), so that if we know that the depth of a pixel is 5, we only have to set $k'=5$ and we will have the 3D coordinates of the point.$\\\\[10pt]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D0Qtw0Pcc57"
   },
   "source": [
    "### From 3D to 2D\n",
    "Let's have a look now at the process of projecting 3D points to 2D. Given any 3D point $\\mathbf{M} = [X,Y,Z]^T \\in \\mathbb{R}^3$, we know that it has a projection point in the image plane $\\mathbf{m} = [x,y]^T \\in \\mathbb{R}^2$.  \n",
    "\n",
    "As seen before, the 3D point in the projection line of $\\mathbf{m}$ with depth $Z$ is:$\\\\[5pt]$\n",
    "\n",
    "$$Z \\begin{bmatrix} x/f  \\\\ y/f \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} Zx/f  \\\\ Zy/f \\\\ Z \\end{bmatrix} \\in \\mathbb{R}^3 \\\\[5pt]$$ \n",
    "\n",
    "So that we can find the 2D image coordinates of the projected point through:$\\\\[5pt]$\n",
    "\n",
    "$$M = \\begin{bmatrix}X \\\\ Y \\\\ Z \\end{bmatrix} =  \\begin{bmatrix} Zx/f  \\\\ Zy/f \\\\ Z \\end{bmatrix} \\longrightarrow \\begin{eqnarray} X = \\frac{Zx}{f}\\ \\rightarrow \\ x = \\frac{fX}{Z} \\\\[3pt] Y = \\frac{Zy}{f} \\ \\rightarrow \\ y = \\frac{fY}{Z} \\end{eqnarray} \\\\[5pt]$$ \n",
    "\n",
    "This way, the relationship between the 3D point and its projected 2D point is like this:$\\\\[10pt]$\n",
    "\n",
    "<center><img src=\"./images/3dto2d.png\" />$\\\\[5pt]$</center>\n",
    "\n",
    "Note that this transformation is not linear, but **it becomes linear if we use homogenous coordinates!**:$\\\\[5pt]$\n",
    "\n",
    "$$\\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}\\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fX \\\\ fY \\\\ Z\\end{bmatrix} \\xrightarrow{\\text{homogenous to Cartesian}} \\begin{bmatrix} fX \\ / \\ Z \\\\ fY \\ / \\ Z \\end{bmatrix} \\\\[5pt]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D0Qtw0Pcc57"
   },
   "source": [
    "### Summary\n",
    "\n",
    "So, these equations relate 3D and 2D coordinates of a certain point in the real world and its projection in the image:\n",
    "\n",
    "<center><img src=\"./images/pinhole_summary.png\"/>$\\\\[5pt]$</center>\n",
    "\n",
    "Note that it is impossible to get the complete 3D position of a certain point from its coordinates in a single image because we need to know the $Z$ coordinate beforehand. That is because the set of 3D points that falls in the line from the optical center to the image point **all of them project at the exact same point in the image**! This is also the reason why **we cannot determine the scale of the objects from a single image**, which is called the **scale indetermination** in monocular vision.\n",
    "\n",
    "Usually, this transformation is made more general by decomposing it as $Z \\tilde{\\mathbf{m}} = \\mathbf{K}_f\\underbrace{\\mathbf{P}_0\\tilde{\\mathbf{M}}_C}_{Z\\tilde{\\mathbf{m}}_1}\\\\[5pt]$, that is:\n",
    "\n",
    "$$Z \\underbrace{\\begin{bmatrix} x  \\\\ y \\\\ 1 \\end{bmatrix}}_{\\tilde{\\mathbf{m}}} = \\underbrace{\\begin{bmatrix} f & 0 & 0\\\\ 0 & f & 0  \\\\ 0 & 0 & 1\\end{bmatrix}}_{\\mathbf{K}_f}\\underbrace{\\begin{bmatrix} 1 & 0 & 0 & 0\\\\ 0 & 1 & 0 & 0  \\\\ 0 & 0 & 1 & 0\\end{bmatrix}}_{\\mathbf{P}_0 = [\\mathbf{I}|\\mathbf{0}]}\\begin{bmatrix} X  \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "In this expression, called **perspective projection equation**, $\\tilde{\\mathbf{m}}$ is the homogeneous 2D point, $\\mathbf{K}_f$ is called the **calibration matrix** and $\\mathbf{P_0}$ is a normalized projection ($f=1$). The image below further illustrate these relations:\n",
    "\n",
    "<center><img src=\"./images/3dto2d-2.png\"/>$\\\\[5pt]$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D0Qtw0Pcc57"
   },
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: From world to image coordinates using the Pinhole model</i></b></span>**\n",
    "\n",
    "**Your first task is** to transform a number of points expressed in `WORLD` coordinates into image ones. You have to consider that:\n",
    "\n",
    "- The world points are given in the  `world` matrix,\n",
    "- both the `WORLD` and the `CAMERA` systems are coincident,\n",
    "- the focal length of the camera is $f = 2.5$, and\n",
    "- you should use **only** linear transformations and express the result in **Cartesian coordinates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WirW-vfNcc59",
    "outputId": "e06b5310-a9fc-492b-a5a6-d68a94adcec9"
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 12.0)\n",
    "\n",
    "# ASSIGNMENT 1 --\n",
    "# World coordinates\n",
    "world = np.array([[-12, 21,-30,41, 67,-54,33,-24,46,-58],  # X\n",
    "                  [ 21,-26,-34,23,-42,-67,76,-54,42,-37],  # Y \n",
    "                  [ 10,  7,  2,13,  4,  7, 5, 15, 8,  7]]) # Z\n",
    "\n",
    "# world = np.array([[12,21,30,41,67,54,33,24,46,58],[21,26,34,23,42,67,76,54,42,37],[10,7,2,13,4,7,5,15,8,7]])\n",
    "# world = np.array([[-12,-21,30,41,-67,54,-33,24,46,-58],[21,-26,34,-23,42,-67,76,-54,-42,37],[10,7,3,13,4,7,5,15,8,7]])\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Prepare figure for 3D data\n",
    "ax = plt.axes(projection='3d')\n",
    "# print(ax.dist)\n",
    "# ax.view_init(azim=-60,elev=-30)\n",
    "\n",
    "# Name axes\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "\n",
    "# Plot points\n",
    "ax.scatter(world[0,:], world[1,:], world[2,:])\n",
    "ax.scatter(0, 0, 0, color=\"red\")\n",
    "ax.view_init(elev=-60, azim=-90)\n",
    "\n",
    "# Focal length\n",
    "f = None\n",
    "\n",
    "# Convert world coordinates to homogenous\n",
    "h_world = np.append(None, np.ones((1,world.shape[1])), axis= 0)\n",
    "\n",
    "# Define transformation matrices\n",
    "K_f = np.array(None)\n",
    "\n",
    "P_0 = np.array(None)\n",
    "\n",
    "# Get sensor homogenous coordinates (apply transformation matrices)\n",
    "h_sensor = None @ None @ None\n",
    "\n",
    "# Transform them to cartesian\n",
    "sensor = None / None\n",
    "\n",
    "# For checking if the result is right\n",
    "print(sensor.round(3))\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Prepare figure\n",
    "ax = fig.gca()\n",
    "\n",
    "# Plot points\n",
    "ax.scatter(None, None)\n",
    "ax.scatter(0, 0, color=\"red\")\n",
    "ax.arrow(0,0,10,0)\n",
    "ax.arrow(0,0,0,10)\n",
    "ax.axis('equal')\n",
    "ax.text(0.485, 0.38, 'y', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "ax.text(0.565, 0.485, 'x', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "ax.invert_yaxis() # to make the plot show the 'y' axis downwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhTYZwjwcc6E"
   },
   "source": [
    "**Check if your results are correct**:\n",
    "\n",
    "<font color='blue'>**Expected output:**  </font>\n",
    "\n",
    "    [[ -3.      7.5   -37.5     7.885  41.875 -19.286  16.5    -4.     14.375  -20.714]\n",
    "     [  5.25   -9.286 -42.5     4.423 -26.25  -23.929  38.     -9.     13.125  -13.214]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORYBVev7cc6E"
   },
   "source": [
    "## 8.4.3 The Camera model <a id=843></a>\n",
    "\n",
    "The Pinhole model is a useful starting point for understanding the processes of image formation, but it still has several limitations because:\n",
    "\n",
    "- the camera frame point $m$ has its coordinates expressed **in meters** within the camera sensor, but we want to know them in **pixels**.\n",
    "- it assumes that the `CAMERA` and the `WORLD` reference systems **are coincident**, which, of course, is not generally the case.\n",
    "\n",
    "This is where we define the **camera model**, which includes some linear transformations for solving these limitations and allows us to address the complete problem of image formation as depicted in the big picture shown before:$\\\\[10pt]$\n",
    "\n",
    "<center><img src=\"images/cameramodel.png\">$\\\\[5pt]$</center>\n",
    "\n",
    "Let's go step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORYBVev7cc6E"
   },
   "source": [
    "### Step 1: From the `WORLD` frame to the `CAMERA` frame\n",
    "\n",
    "This transformation implies finding a rotation matrix $\\mathbf{R}$ and a translation vector $\\mathbf{t}$ that **relates the position and orientation of the camera w.r.t. the world**. This is a 3D to 3D transformation and we have used it in the previous notebooks.\n",
    "\n",
    "<center><img src=\"images/world_camera.png\">$\\\\[5pt]$</center>\n",
    "\n",
    "As said before, usually the camera is not at the world center, mainly because the camera will move within the world, or even because will be more than one camera in our perception system.\n",
    "\n",
    "Remember that in the second notebook of this chapter, we learned how to apply a rotation and a translation to a set of 3D points using homogeneous transformations:$\\\\[5pt]$\n",
    "\n",
    "$$\\mathbf{M}_C = \\mathbf{R}\\mathbf{M}_W + \\mathbf{t} \\ \\xrightarrow{\\text{In homogenous}} \\ \\tilde{\\mathbf{M}}_C = \\mathbf{D} \\tilde{\\mathbf{M}}_W$$\n",
    "\n",
    "If we add this transformation to the Pinhole model, the new **perspective projection equation** takes the expression:\n",
    "\n",
    "$$\\lambda \\tilde{\\mathbf{m}} = \\mathbf{K}_f \\mathbf{P}_0 \\tilde{\\mathbf{M}}_C = \\mathbf{K}_f \\mathbf{P}_0 \\mathbf{D} \\tilde{\\mathbf{M}}_W$$\n",
    "\n",
    "which allow us to transform points in 3D within the `WORLD` reference system (in meters) to points in 2D in the image sensor (**also in meters!**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORYBVev7cc6E"
   },
   "source": [
    "### Step 2: From the sensor image to a computer image\n",
    "\n",
    "Once we have obtained the sensor image points, we want to get the **image matrix coordinates** of such points **in pixels** (that is, *row and column* within the image matrix), which are the units we use in the computer in all the artificial vision processes we have studied in this book.$\\\\[10pt]$\n",
    "\n",
    "<center><img src=\"images/sensor_matrix.png\">$\\\\[5pt]$</center>\n",
    "\n",
    "As you can see in the figure, both images are related by an **affine** transformation (i.e. a 2D homography as seen in the previous notebook). As we know, affine transformations allow **rotation + translation + scale** (different for each axis). In this case we don't need any rotation, but axes **do scale**, and such scale represents the size in meters in the sensor of each pixel in the image:\n",
    "\n",
    "$$\\mathbf{m}' = \\mathbf{A}\\mathbf{m} + \\mathbf{b} = \\begin{bmatrix}k_x & 0 \\\\ 0 & k_y\\end{bmatrix}\\underbrace{\\begin{bmatrix} x \\\\ y \\end{bmatrix}}_{\\text{meters}} + \\underbrace{\\begin{bmatrix} u_0 \\\\ v_0 \\end{bmatrix}}_{\\text{pixels}} \\xrightarrow{\\text{In homogenous}} \\begin{bmatrix} u \\\\ v\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} k_x & 0 & u_0  \\\\ 0 & k_y & v_0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y\\\\ 1 \\end{bmatrix} \\qquad \\bf{\\tilde{\\mathbf{m}}' = \\mathbf{K}_s \\tilde{\\mathbf{m}}}$$\n",
    "\n",
    "Note that in this expression, $k_x$ and $k_y$ determine the scale and have units of **pixels/meter**, while $u_0$ and $v_0$ are the coordinates of the **principal point**, which is the projection of the $Z$ axis in the image plane. These elements are part of the **intrinsic parameters** of the camera, which means that they are always **constant** for the same camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORYBVev7cc6E"
   },
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: From image coordinates to pixels</i></b></span>**\n",
    "\n",
    "It is time to transform the sensor coordinates (in meters) obtained in the previous assignment to proper image coordinates (in pixels). For this, you will need the intrinsic parameters of the camera, which for this example are: \n",
    "\n",
    "- `k_x = 2`\n",
    "- `k_y = 3`\n",
    "- `u_0 = 300`\n",
    "- `v_0 = 200`\n",
    "\n",
    "Don't worry, these values are normally provided by the manufacturer of the camera, although not everything is lost otherwise. We could also calculate them through a process called **camera calibration**, which we will address in the next chapter. Exciting!\n",
    "\n",
    "Concretely, **you have to**:\n",
    "- define the homography $\\mathbf{K}_s$ (`H` in the code) relating meters with pixels,\n",
    "- apply it to the sensor coordinates in `h_sensor` as retrieved in the previous assignment, \n",
    "- transform the resulting coordinates to Cartesian ones, and\n",
    "- show them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s87Cg_FZcc6G",
    "outputId": "c989ba23-4731-4148-8d7e-9b107d08ffb4"
   },
   "outputs": [],
   "source": [
    "# ASSIGNMENT 2 --\n",
    "# Define intrinsic parameters\n",
    "kx = None \n",
    "ky = None \n",
    "u0 = None \n",
    "v0 = None\n",
    "\n",
    "# Write your code here!\n",
    "\n",
    "# Transformation matrix\n",
    "H = np.array(None)\n",
    "\n",
    "# Transform sensor coordinates to image coordinates\n",
    "h_image_pixels = None @ None\n",
    "center = H @ np.array([0,0,1])\n",
    "\n",
    "# Transform to cartesian\n",
    "image_pixels = None / None\n",
    "\n",
    "# For checking if the result is right\n",
    "print(image_pixels.astype(int))\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Prepare figure for 3D data\n",
    "ax = fig.gca()\n",
    "\n",
    "# Plot points\n",
    "ax.scatter(None, None, marker='s')\n",
    "ax.scatter(center[0], center[1], color=\"red\")\n",
    "\n",
    "# Set axes limits\n",
    "ax.set_xlim(0, 600)\n",
    "ax.set_ylim(0, 400)\n",
    "#ax.axis('equal')\n",
    "ax.invert_yaxis() # to make the plot show the 'y' axis downwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xcy8_K_cc6M"
   },
   "source": [
    "You can **check if your results are correct**:\n",
    "\n",
    "<font color='blue'>**Expected output:**  </font>\n",
    "\n",
    "    [[294 315 225 315 383 261 333 292 328 258]\n",
    "     [215 172  72 213 121 128 314 173 239 160]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pSQvQb-cc6N"
   },
   "source": [
    "### General form of the Perspective matrix\n",
    "\n",
    "Let's see how the final perspective transformation would be **from 3D world frame to computer image**:\n",
    "\n",
    "$$\\lambda \\begin{bmatrix} u \\\\ v\\\\ 1 \\end{bmatrix} = \\underbrace{\\begin{bmatrix} k_x & 0 & u_0  \\\\ 0 & k_y & v_0 \\\\ 0 & 0 & 1 \\end{bmatrix}}_{\\text{from sensor to image}}\\underbrace{\\begin{bmatrix} f & 0 & 0  \\\\ 0 & f & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\begin{bmatrix} 1 & 0 & 0 & 0  \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}}_{\\text{from camera frame to sensor}}\\underbrace{\\begin{bmatrix} \\bf{R} & \\bf{t}  \\\\ \\bf{0^T_3} & 1 \\end{bmatrix}}_{\\text{from world to camera frame}} \\begin{bmatrix} X_W \\\\ Y_W\\\\ Z_W \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "Again, we can merge some transformations:\n",
    "\n",
    "$$\\lambda \\begin{bmatrix} u \\\\ v\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f k_x & 0 & u_0  \\\\ 0 & f k_y & v_0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} \\bf{R} & \\bf{t}\\end{bmatrix} \\begin{bmatrix} X_W \\\\ Y_W\\\\ Z_W \\\\ 1 \\end{bmatrix} \\qquad \\bf{\\lambda \\tilde{m}' = K \\left[ R\\ t \\right] \\tilde{M}_W}$$\n",
    "\n",
    "where \n",
    "- $\\bf{R}$ and $\\bf{t}$ are **extrinsic** paremeters that depend on the camera position.\n",
    "- $f$, $\\ k_x$, $\\ k_y$, $\\ u_0$ and $v_0$ are **intrinsic** parameters (constant) that depend on the camera that is being used.\n",
    "\n",
    "*Note: you will usually find that $f$ is directly expressed in pixels, so actually it will be the result of $fk_x$ and $fk_y$, and furthermore, this is sometimes also called $fk_x = s_x$ and $fk_y = s_y$*\n",
    "\n",
    "Here you have the full camera model in action:$\\\\[10pt]$\n",
    "\n",
    "<img src=\"images/camera_model.png\" width=\"700\">$\\\\[5pt]$\n",
    "\n",
    "**For a visual explanation of this model, you can check** [this interactive application](http://ksimek.github.io/2012/08/22/extrinsic/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMkOkxNVcc6N"
   },
   "source": [
    "## 8.4.4 Putting things to work: the RGB-D  image <a id=\"844\"></a>\n",
    " \n",
    "As a practical exercise, we are going to apply the pinhole camera model to get a 3D set of points (also known as a **pointcloud**) from an RGB-D image. Remember that we said that it is not possible to determine the 3D position of a point from just its coordinates in a single image? Well, in this case we have **the depth information** included in the image and therefore we can now accomplish that! So, let's transform 2D points in the RGB image to 3D points in the world frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMkOkxNVcc6N"
   },
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 3a: Visualizing our RGB-D image</i></b></span>**\n",
    "\n",
    "First of all, show the RGB-D image in a 2x1 subplot with the RGB part in the left (`person_rgb.png`) and the depth part in the right (`person_depth.png`) and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2QPUHvucc6P",
    "outputId": "2ed4c6d5-189c-40cd-a323-9cab6322c9ba"
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12.0, 12.0)\n",
    "\n",
    "# ASSIGNMENT 3a --\n",
    "# Write your code here!\n",
    "\n",
    "# Read images\n",
    "image = cv2.imread(images_path + None,-1)\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "depth = cv2.imread(images_path + None,0)\n",
    "\n",
    "# Show RGB\n",
    "plt.subplot(121)\n",
    "plt.title(\"RGB\")\n",
    "plt.imshow(None)\n",
    "\n",
    "# Show depth\n",
    "plt.subplot(122)\n",
    "plt.title(\"Depth\")\n",
    "plt.imshow(None, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwXCHmwTcc6V"
   },
   "source": [
    "### From the RGB-D image to 3D coordinates \n",
    "\n",
    "As you can see, the depth image represents the depth with levels of grey color, the darker it is a pixel, the closer to the camera it is. So, for each pixel **there is a mapping between intensity $\\rightarrow$ depth**. Since we have the actual depth of the points, we just need to get the projection lines of the pixels in the image and then select the Z of the point according to the depth.\n",
    "\n",
    "First, we need to transform the points in **image coordinates** to **sensor coordinates**. For this, we reverse the sensor to image method seen previously, either by using the pseudoinverse matrix or by **isolating the variables**.\n",
    "\n",
    "We know that *(assuming that the scale is 1)*:\n",
    "\n",
    "$$\\bf{2D \\rightarrow 2D} \\qquad \\underbrace{\\begin{bmatrix} u \\\\ v\\\\ 1 \\end{bmatrix}}_{\\text{pixels}} = \\begin{bmatrix} f & 0 & u_0  \\\\ 0 & f & v_0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\underbrace{\\begin{bmatrix} x \\\\ y\\\\ 1 \\end{bmatrix}}_{\\text{meters}}$$\n",
    "\n",
    "If we isolate $x$ and $y$, we get the coordinates in the sensor (note that since we are already dividing by $f$, this coordinates are expressed in a projective plane at $Z=1$):\n",
    "\n",
    "$$\\begin{eqnarray} x = \\frac{u-u_0}{f}, \\; y = \\frac{v-v_0}{f} \\end{eqnarray}$$\n",
    "\n",
    "Finally, we add the depth component $Z$, which is available in the depth image:\n",
    "\n",
    "$$\\bf{2D \\rightarrow 3D} \\; \\  \\begin{bmatrix} X \\\\ Y\\\\ Z \\end{bmatrix} = Z \\begin{bmatrix} x \\\\ y\\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "so $X=Zx$ and $Y=Zy$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwXCHmwTcc6V"
   },
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 3b: Building a point cloud from an RGB-D image</i></b></span>**\n",
    "\n",
    "Generate a $[3\\times N]$ matrix with the 3D camera frame coordinates of all pixels in the image `person_rgb.png` using these intrinsic parameters:\n",
    "\n",
    "- `f = 525`\n",
    "- `k_x = 1`\n",
    "- `k_y = 1`\n",
    "- `u_0 = 319.5`\n",
    "- `v_0 = 239.5`\n",
    "\n",
    "Notice that we are using a linear scale that discretizes the camera operating range, from 0 up to 5 meters, in order to encode such distances in the 256 pixel possible values of a greyscale image. In this way, `scale=0.02` (approx. $5m/256$) so, for example, if the value of a pixel in the depth image is 20, it means that its corresponding 3D point is `20*scale=0.4` meters away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_k7yNxFUcc6W"
   },
   "outputs": [],
   "source": [
    "# Read images\n",
    "image = cv2.imread(images_path + \"person_rgb.png\",-1)\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "depth = cv2.imread(images_path + \"person_depth.png\",0)\n",
    "\n",
    "# Import intrinsic parameters\n",
    "f  = None\n",
    "u0 = None\n",
    "v0 = None\n",
    "\n",
    "# Matrix for storing (homogeneous) sensor coordinates\n",
    "h_sensor = np.zeros((3,image.shape[0]*image.shape[1]))\n",
    "\n",
    "# Get sensor coordinates in meters\n",
    "point = 0\n",
    "for v in range(image.shape[0]): # rows\n",
    "    for u in range(image.shape[1]): # columns\n",
    "        # Transform to sensor coordinates\n",
    "        h_sensor[0,point] = None\n",
    "        h_sensor[1,point] = None\n",
    "        h_sensor[2,point] = 1 # Homogeneous coordinates\n",
    "        point += 1\n",
    "        \n",
    "scale = 0.02 # ~5m/256 \n",
    "image_d_fila = np.reshape(depth,(1,depth.shape[0]*depth.shape[1]))\n",
    "image_d_fila = image_d_fila*scale\n",
    "\n",
    "map_3d = h_sensor * image_d_fila # element-wise multiplication\n",
    "\n",
    "# check coordinates at points 100060, 100061 and 100062\n",
    "print(map_3d[:,100060:100063])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rtva6_aUcc6f"
   },
   "source": [
    "You can **check if your results are correct** (Positions of the points nÂº 100060, 100061 and 100062 in the camera frame):\n",
    "\n",
    "<font color='blue'>**Expected output:**  </font>\n",
    "\n",
    "    [[-0.30702857 -0.30394286 -0.30085714]\n",
    "     [-0.25765714 -0.25765714 -0.25765714]\n",
    "     [ 1.62        1.62        1.62      ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7y4fHCwcc6h"
   },
   "source": [
    "This is what you should get:\n",
    "\n",
    "<img src=\"images/pointcloud.png\" >$\\\\[5pt]$\n",
    "\n",
    "You can see how all the pixels in the image have been back-projected to 3D and we have a fancy pointcloud showing our scene in 3D. Now all those points are in the `WORLD` reference system (we assumed that `WORLD`and `CAMERA` reference systems were coincident) and we can move our camera in order to get an image from a different perspective. Let's go for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6KuWgEGcc6h",
    "outputId": "cc0d621c-0640-4c44-9d92-264f52029024"
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12.0, 12.0)\n",
    "\n",
    "# Let's draw the pointcloud!\n",
    "%matplotlib widget\n",
    "plot3DScene(map_3d,image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what you should get:\n",
    "\n",
    "<img src=\"images/pointcloud.png\" >$\\\\[5pt]$\n",
    "\n",
    "You can see how all the pixels in the image have been back-projected to 3D and we have a fancy pointcloud showing our scene in 3D. Now all those points are in the `WORLD` reference system and we can move our camera in order to get an image from a different perspective. Let's go for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zsCTE1Olcc6n"
   },
   "source": [
    "### Moving the camera and observing the scene\n",
    "\n",
    "So, in this last part of this practical exercise we are going to move the camera position and project the pointcloud again to form a new 2D image of it. This implies:\n",
    "- there is a rotation and translation between the `WORLD` and `CAMERA` reference systems, so we need to compute the point coordinates in the latter,\n",
    "- then, we need to project those points to the sensor plane,\n",
    "- then, we need to scale and translate such points in the sensor plane to an *computer image plane*,\n",
    "- and finally, we need to adjust their coordinates to get the image plane with origin at the top-left.\n",
    "\n",
    "Therefore, you should use the full projective transformation from the world reference frame to the image:\n",
    "\n",
    "$$\\lambda \\begin{bmatrix} u \\\\ v\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f  & 0 & u_0  \\\\ 0 & f  & v_0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} \\bf{R} & \\bf{t}\\end{bmatrix} \\begin{bmatrix} X_W \\\\ Y_W\\\\ Z_W \\\\ 1 \\end{bmatrix} \\qquad \\bf{\\lambda \\tilde{m}' = K \\left[ R\\ t \\right] \\tilde{M}_W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zsCTE1Olcc6n"
   },
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 3c: Taking a picture</i></b></span>**\n",
    "\n",
    "Let's change the pose `CAMERA` in the `WORLD` reference system and take a new picture of the 3D scene. For that: \n",
    "\n",
    "- Define a rotation of $15Âº$ in the $Y$ axis and a translation of $0.5m$ in the $Z$ axis to move the camera. \n",
    "- Then, apply the complete camera model to the previous computed 3D pointcloud `h_map3D`.\n",
    "- Iterate over the projected points for checking if they must appear in the image:\n",
    "    - their $u$ and $v$ coordinates are within the image dimensions, and\n",
    "    - since multiple points can be projected into the same pixel, keep the point that is closer to the camera.\n",
    "- Finally, show the original image and the new one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcpfOjcocc6o",
    "outputId": "383448c6-9210-4344-ec95-f00548fb1449"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# intrinsic parameters\n",
    "f  = None\n",
    "u0 = None\n",
    "v0 = None\n",
    "\n",
    "# Define transformation matrices\n",
    "angle = np.radians(None)\n",
    "R = np.array(None) # rotation\n",
    "\n",
    "T = np.zeros((3,4))\n",
    "T[0:3,0:3] = R\n",
    "T[0:3,3] = None # translation\n",
    "\n",
    "K = np.array(None)\n",
    "\n",
    "# Transform map to homogenous coordinates\n",
    "h_map3D = np.append(map_3d, np.ones((1,map_3d.shape[1])), axis=0)\n",
    "\n",
    "# Apply transformation\n",
    "h_new_image = None @ None @ None\n",
    "\n",
    "# Transform to cartesian (they may appear zeroes in z!)\n",
    "proj = np.divide(h_new_image[:2,:], h_new_image[2,:], where=h_new_image[2,:]!=0)\n",
    "proj[np.isnan(proj)] = 0 # Fix division by 0\n",
    "proj = proj.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQEaZNIucc6s",
    "outputId": "9eb49813-ffe2-4201-aff0-61f46f2234af"
   },
   "outputs": [],
   "source": [
    "# Construct new image\n",
    "new_image = np.zeros_like(image)\n",
    "new_depth = np.full(depth.shape,np.inf)\n",
    "\n",
    "image_vector = image.reshape(image.shape[0]*image.shape[1],3) # this is a Nx3 vector with the all pixels in RGB\n",
    "\n",
    "# iterate over the projected points and check if they must appear in the image\n",
    "for p in range(proj.shape[1]):\n",
    "        u,v = proj[:,p]        \n",
    "        z = h_map3D[2,p] # z-coordinate\n",
    "        # Should this pixel appear in the image?\n",
    "        if (u>=None) and (u<None]) and (v>=None) and (v<None): # Check if pixel is in bounds\n",
    "            if (new_depth[v,u] > z): # Check if pixel is closer than other in that position\n",
    "                new_depth[v,u] = None\n",
    "                new_image[v,u,:] = image_vector[p,:] # Get the color\n",
    "            \n",
    "# Show original image\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(13, 13))\n",
    "ax1.imshow(None)\n",
    "ax1.set_title(\"Original RGB image\")\n",
    "ax2.imshow(None)\n",
    "ax2.set_title(\"New image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksWpXa_-cc6y"
   },
   "source": [
    "This is the resulting image you should have obtained:\n",
    "\n",
    "<img src=\"images/new_render.png\">$\\\\[5pt]$\n",
    "\n",
    "You can see how now we have a new perspective of the point cloud! Cool, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n",
    "\n",
    "**Now you are in a good position to answer these questions:**\n",
    "\n",
    "- If we have a RGB-D camera with an operating range from 0 up to 10 meters, what would be the scale used to codify distances in a greyscale image?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- If the scale is of 0,01, which would be the maximum operating range? Assume that the operating range starts at 0.\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "  \n",
    "- The following image have been taken with the camera rotated in the $Y$ axis. Which has been the rotated angle? $30Âº$ or $-30Âª$?\n",
    "\n",
    "    <img src=\"images/new_render_y_30.png\">$\\\\[5pt]$\n",
    "    \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuEpxbsTcc6y"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Brilliant! Although this has been a dense chapter, it has been satisfying once you get it. In this notebook, you have learned:\n",
    "\n",
    "- the principle of RGB-D images (widely used in computer vision and robotics fields nowadays),\n",
    "- how the pinhole camera model works,\n",
    "- how a more complete camera model works,\n",
    "- how to project a 3D set of points to a 2D image,\n",
    "- how to back-project an image with depth information to the 3D space to get a pointcloud."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "6.4 The Camera model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "248.85px",
    "left": "1482px",
    "right": "20px",
    "top": "120px",
    "width": "351px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
