{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Bayesian Classifier based on Normal Distribution\n",
    "\n",
    "In this second notebook of the object recognition chapter we will explore a popular statistical classifiers, the Bayesian ones. These methods are based on the idea that, given a number of features characterizing an object (the famous vector $\\mathbf{x}$), the Bayes' rule can be used to predict its class. Recall the Bayes' rule: $\\\\[10pt]$\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/bayes2.png\">\n",
    "</center>\n",
    "\n",
    "In this way, Bayesian classifiers build probabilistic (statistical) models of the features according to certain training data, and use them to classify new objects. \n",
    "\n",
    "In this notebook, will explain:\n",
    "- how (Naïve) Bayesian classifiers work (<a href=\"#721\">section 7.2.1</a>) and, \n",
    "- specifically, how to classify feature vectors supposing that they follow a normal distribution (<a href=\"#722\">section 7.2.2</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem context - Traffic sign recognition\n",
    "\n",
    "In the previous notebook, *AliquindoiCars* contacted us looking for a TSR technique to be integrated into a self-driving car. They provided us with some segmented images of traffic signs that their autonomous cars captured during test drivings. These images are located in `images/circles/` containing circled signs, `images/triangles` containing signs having triangular shapes and `images/squares` containing signs having square shapes.$\\\\[3pt]$\n",
    "\n",
    "<img src=\"./images/signs.jpg\" width=\"400\">$\\\\[3pt]$\n",
    "\n",
    "Previously, we extracted a feature vector from each image using Hu moments. Now, we will train a classifier using the feature vectors we saved the in previous notebook. For classification, there are many methods we can apply, such as [kNN algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm), [random forest](https://en.wikipedia.org/wiki/Random_forest), etc. In this notebook, we will explore and use a classical one, the Naïve Bayes classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.interpolate\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "\n",
    "images_path = './images/'\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.PlotEllipse import PlotEllipse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 (Naïve) Bayesian classifier <a id=\"721\"></a>\n",
    "\n",
    "The simplest case of these classifiers is the **Naïve Bayesian** one, which considers the strong (naïve) independence assumption that the input features are conditionally independent of each other given the object class. That is, for example, if we are using compactness and extent to describe objects, this classifier assumes that the value for both features is not related if the object class is known, i.e. circle. For the visual system in our previous notebook, which is in charge of recognizing objects in a kitchen, this means that if it knows that an object is a spoon, then its compactness would be unrelated with its extent (we could not say anything about its possible extent given a certain compactness, for example). \n",
    "\n",
    "In a general case with an arbitrary number of features $n$, the Bayes' rule can be written as:\n",
    "\n",
    "$$P(C|x_1,\\dots,x_n)= \\frac{P(x_1,\\dots,x_n|C)P(C)}{P(x_1,\\dots,x_n)}$$\n",
    "\n",
    "if the following naïve conditional assumption:\n",
    "\n",
    "$$P(x_i|C,x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)=P(x_i|C)$$\n",
    "\n",
    "is considered for each feature $i$, then the rule can be simplified to:\n",
    "\n",
    "$$P(C|x_1,\\dots,x_n)=\\frac{P(C)\\prod_{i=1}^n P(x_i|C)}{P(x_1,\\dots,x_n)}$$\n",
    "\n",
    "Given that $P(x_1,\\dots,x_n)$ is constant for a certain input, and that we are looking for the class that has the highest posterior probability, the following classification rule is considered:\n",
    "\n",
    "$$P(C|x_1,\\dots,x_n) \\propto P(C) \\prod_{i=1}^{n} P(x_i|C)$$\n",
    "\n",
    "$$\\hat{C}= arg\\,\\underset{C}max  P(C) \\prod_{i=1}^{n} P(x_i|C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing, having a set of features describing an object $\\mathbf{x} = [x_1, x_2, x_3, \\ldots, x_n]^T$ and a set of possible belonging classes $C = [C_1, C_2, C_3, \\ldots, C_n]$, the Bayesian classifier assigns $\\mathbf{x}$ to the class $C_i$ that has the highest posterior probability $P(C_i/x)$ (the more probable, the less probability of making a mistake). This is called a **MAP** (Maximum A Posteriori) prediction.\n",
    "\n",
    "Considering the following joint probability distribution $P(C,\\mathbf{x})$ (in this case $\\textbf{x}$ contains just one variable):$\\\\[3pt]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 2, 3])\n",
    "pC1_given_x = np.array([0.3, 0.4, 0.8, 0.9])\n",
    "pC2_given_y = np.array([0.7, 0.6, 0.2, 0.1])\n",
    "\n",
    "x_new = np.linspace(0, 3, 300)\n",
    "a_BSpline = scipy.interpolate.make_interp_spline(x, pC1_given_x, bc_type=\"natural\")\n",
    "b_BSpline = scipy.interpolate.make_interp_spline(x, pC2_given_y, bc_type=\"natural\")\n",
    "pC1_given_x_new = a_BSpline(x_new)\n",
    "pC2_given_y_new = b_BSpline(x_new)\n",
    "\n",
    "plt.plot(x_new, pC1_given_x_new, label='p(C1|x)')\n",
    "plt.plot(x_new, pC2_given_y_new, label='p(C2|x)')\n",
    "plt.plot([0.48,0.48],[0, 0.687],'g'); plt.plot(0.48,0.689,'og', label=\"p(C1|x=0.5)\")\n",
    "plt.plot([0.52,0.52],[0, 0.32],'r'); plt.plot(0.52,0.315,'or', label=\"p(C2|x=0.5)\")\n",
    "\n",
    "plt.ylabel('P(C,x)')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([0,3])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this MAP classification corresponds to assign the object to the class $C_i$ with the highest value. Since such an object is characterized by $x=x_1=0.5$, it is assigned to $C_2$ (assuming equal prior probability for each class), the category with highest $P(C|x=x_1)$ value. \n",
    "\n",
    "Depending on the application, it is convenient to consider a *rejection region*, where none probability is high enough and no decision is made (e.g. $P(C_1|x) = P(C_2|x) = 0.5$). The following code illustrates this, where a probability threshold $\\theta$ defining such a region:$\\\\[3pt]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_new, pC1_given_x_new, label='p(C1|x)')\n",
    "plt.plot(x_new, pC2_given_y_new, label='p(C2|x)')\n",
    "\n",
    "plt.ylabel('P(C,x)')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([0,3])\n",
    "plt.legend();\n",
    "\n",
    "theta = 0.1 # threshold\n",
    "\n",
    "plt.plot([0, 3],[0.5+theta, 0.5+theta],'g--')\n",
    "plt.plot([0, 3],[0.5-theta, 0.5-theta],'g--')\n",
    "plt.plot([1, 1],[0, 0.6],'g')\n",
    "plt.plot([1.5, 1.5],[0, 0.6],'g')\n",
    "plt.text(1.1, 0.1, 'Rejection \\n region');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n",
    "\n",
    "Now that you have notions about the rejection region, **answer the following questions**:\n",
    "\n",
    "- Which class would an object with `x_1=0.5` belong to?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- Which class would an object with `x_1=1.2` belong to?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- Would the threshold theta be the same in any application?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building discriminant functions\n",
    "\n",
    "In the same way as we built linear and generalized discriminant functions in the previous notebook, those functions can be also defined to design a Bayesian classifier. For that, the goal is to obtain a discriminant function $d_i(x)$ for each class $C_i$, such that $d_i(x) \\gt d_j(x)$ whenever $P(C_i|x) \\gt P(C_j|x)$. Let's design them step by step!\n",
    "\n",
    "From the Bayes' rule we have that :$\\\\[5pt]$\n",
    "\n",
    "$$d_k(x) = P(C_k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|C_k)P(C_k)}{P(\\mathbf{x})}\\\\[5pt]$$\n",
    "\n",
    "This is, Bayes' rule defines a way to compute the posterior probability $P(C_k|\\mathbf{x})$ of a feature vector $\\mathbf{x}$ for each class $C_k$. We can further simplify this function:$\\\\[5pt]$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "&d_k(x) &= P(C_k/\\mathbf{x}) = \\frac{p(\\mathbf{x}|C_k)P(C_k)}{P(\\mathbf{x})} \n",
    "\\\\\n",
    "{P(\\mathbf{x})\\text{ is a constant value }\\forall k} \\Big\\Downarrow\n",
    "\\\\ \n",
    "\\hspace{0.2cm}&d_k(x) &= p(\\mathbf{x}|C_k)P(C_k)\n",
    "\\\\\n",
    "\\max \\ln(f(\\mathbf{x})) = \\max f(\\mathbf{x}) \\Big\\Downarrow\n",
    "\\\\\n",
    "&d_k(x) &= \\ln p(\\mathbf{x}|C_k) + \\ln P(C_k)\n",
    "\\\\\n",
    "\\text{If } P(C_k) = P(C_j)\\; \\forall j,k  \\Big\\Downarrow\n",
    "\\\\\n",
    "&d_k(x) &= lnp(\\mathbf{x}|C_k)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The resulting formulation is also called **MLE (Maximum log-Likelihood Estimation)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Naive Bayesian classifier for normal distribution <a id=\"722\"></a>\n",
    "\n",
    "The different Naïve Bayesian Classifiers differ in the probability distribution considered for modeling $P(x_i|C)$. In this section we will cover **Normal distribution based ones**, which suppose that input features follow the probability density function of a Gaussian distribution:$\\\\[5pt]$\n",
    "\n",
    "$$p(\\mathbf{x}|C_i) = \\large{\\frac{1}{(2\\pi)^{n/2}|\\Sigma^i|^{1/2}}}\n",
    "e^{-\\frac{1}{2}(\\mathbf{x}-\\mu^i)^T\\Sigma_i^{-1}(\\mathbf{x}-\\mu^i)}\\\\[5pt]$$\n",
    "\n",
    "so two set of parameters are considered:$\\\\[5pt]$\n",
    "\n",
    "- A mean vector for each class: $\\mathbf{\\mu}=[\\mu_1 \\mu_2 \\dots \\mu_f \\dots \\mu_n]^T$\n",
    "- A covariance matrix for each class:\n",
    "$\\hspace{5pt}\n",
    "\\small\n",
    "\\Sigma = E[(\\mathbf{x}-\\mathbf{\\mu}) \\cdot (\\mathbf{x}-\\mathbf{\\mu})^T]\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{11} & \\cdots & \\sigma_{1f} & \\cdots & \\sigma_{1n} \\\\\n",
    "\\vdots & & \\vdots & &  \\vdots \\\\\n",
    "\\sigma_{n1} & \\cdots & \\sigma_{nf} & \\cdots & \\sigma_{nn}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "\\sigma_{11} & \\cdots & 0 & \\cdots & 0 \\\\\n",
    "\\vdots & & \\vdots & &  \\vdots \\\\\n",
    "0 & \\cdots & 0 & \\cdots & \\sigma_{nn}\n",
    "\\end{bmatrix}}_{\\text{Independence assumption (Naïve Bayes)}}\n",
    "$\n",
    "\n",
    "The simplification of the covariance matrix can be done by assuming independence (not correlation) among features, that is, the assumption done by Naïve Bayes!\n",
    "\n",
    "The following code is just a review about how a Gaussian distribution with two variables depends on its two parameters $\\mathbf{\\mu}$ and $\\Sigma$. Feel free to change such parameters and experience their influence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_gaussian(fig, rv, x, y, pos, position):\n",
    "    \"\"\" Plot 2d contours of a 3D gaussian   \n",
    "    \"\"\"\n",
    "    label = \"rv\" + str(position)\n",
    "    position = str(23)+str(position)\n",
    "    ax = fig.add_subplot(int(position))\n",
    "    ax.contourf(x, y, rv.pdf(pos))    \n",
    "    ax.text(-4,-4,label,bbox=dict(facecolor='white', alpha=0.5))\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "x, y = np.mgrid[-5:5:.01, -5:5:.01]\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "# Define 6 different Gaussian distributions \n",
    "mean1 = np.array([2.0, 1.0])\n",
    "covar1 = np.array([[0.5, 0.0],[0.0, 0.5]])\n",
    "rv1 = multivariate_normal(mean1, covar1)\n",
    "\n",
    "mean2 = np.array([-2.0, 2.0])\n",
    "covar2 = np.array([[0.3, 0.0],[0.0, 1.8]])\n",
    "rv2 = multivariate_normal(mean2, covar2)\n",
    "\n",
    "mean3 = np.array([1.0, 0.0])\n",
    "covar3 = np.array([[0.8, 0.7],[0.7, 1.3]])\n",
    "rv3 = multivariate_normal(mean3, covar3)\n",
    "\n",
    "mean4 = np.array([2.0, -1.6])\n",
    "covar4 = np.array([[2.0, 0.0],[0.0, 2.0]])\n",
    "rv4 = multivariate_normal(mean4, covar4)\n",
    "\n",
    "mean5 = np.array([0.0, 0.0])\n",
    "covar5 = np.array([[4.0, 0.0],[0.0, 1.8]])\n",
    "rv5 = multivariate_normal(mean5, covar5)\n",
    "\n",
    "mean6 = np.array([0.0, 2.0])\n",
    "covar6 = np.array([[3.9, -0.5],[-0.5, 1.1]])\n",
    "rv6 = multivariate_normal(mean6, covar6)\n",
    "\n",
    "# Show the contours fo the 3D gaussians\n",
    "fig = plt.figure()\n",
    "plot_2d_gaussian(fig,rv1,x,y,pos,1)\n",
    "plot_2d_gaussian(fig,rv2,x,y,pos,2)\n",
    "plot_2d_gaussian(fig,rv3,x,y,pos,3)\n",
    "plot_2d_gaussian(fig,rv4,x,y,pos,4)\n",
    "plot_2d_gaussian(fig,rv5,x,y,pos,5)\n",
    "plot_2d_gaussian(fig,rv6,x,y,pos,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the discriminant function\n",
    "\n",
    "It is time to see how we could define the discriminant function $d_k(\\mathbf{x})$ for this classifier:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "d_k(\\mathbf{x}) &=& \n",
    "\\ln P(C_k) + \\ln p(\\mathbf{x}|C_k) = \\ln P(C_k) + \\ln \\frac{1}{(2\\pi)^{n/2}|\\Sigma^k|^{1/2}}e^{-\\frac{1}{2}\\overbrace{\\color{blue}{(\\mathbf{x}-\\mu^k)^T\\Sigma_k^{-1}\\mathbf{x}-\\mu^k)}}^{\\text{Squared Mahalanobis distance}}}\\\\[7pt]\n",
    "    &=& \n",
    "\\ln P(C_k) - \\ln 2\\pi^{n/2} - \\ln |\\Sigma^k|^{1/2} - \\frac{1}{2} \\color{blue}{D_k^2(x)}\\\\[7pt]\n",
    "    &=& \n",
    "\\ln P(C_k) - \\frac{1}{2}\\left[\\underbrace{n \\ln 2\\pi}_{\\text{constant}} +  \\ln |\\Sigma^k| + D_k^2(x)\\right] \\\\[7pt]\n",
    "  &=&\n",
    "\\ln P(C_k) - \\frac{1}{2}\\left[\\ln |\\Sigma^k| + D_k^2(x)\\right]\\\\[7pt]\n",
    "\\end{eqnarray}\n",
    "\\\\[8pt]$$\n",
    "\n",
    "We can see that the resulting **discriminant function is quadratic**:\n",
    "\n",
    "$$\\small\n",
    "d_k(\\mathbf{x}) \n",
    "= \n",
    "ln\\ P(C_k) - \\underbrace{ \\frac{1}{2} \\left[ ln\\ |\\Sigma^k| + \\mu^{kT} (\\Sigma^k)^{-1} \\mu^k \\right]}_{\\text{Independent term}} \n",
    "+\n",
    "\\underbrace{\\mathbf{x}^T (\\Sigma^k)^{-1}\\mathbf{\\mu}^k}_{\\text{linear term}} \n",
    "-\n",
    "\\frac{1}{2}\\underbrace{ \\mathbf{x}^T (\\Sigma^k)^{-1}\\mathbf{x}}_{\\text{Quadratic term}}\n",
    "$$\n",
    "\n",
    "Visually, the **division boundaries are parabolas**:\n",
    "\n",
    "<img src=\"./images/visual_normal_bayes.png\">$\\\\[3pt]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1a: Training the classifier</i></b></span>**\n",
    "\n",
    "Now, we are going to implement this Naïve Bayes classifier for normal distributions **using the Hu moments** computed in the previous exercise.\n",
    "\n",
    "The first step for training such classifier is computing the weights' matrix of the discriminant function. In this case, it depends on the **mean** ($\\bf{\\mu}$, dimension $(2,)$) and **covariance matrix** ($\\Sigma$, dimension $(2,2)$), which can be retrieved from the training data through MLE. \n",
    "\n",
    "In the previous notebook we proved that our TSR problem can be solved using only the first and second Hu moments. In this one **your first task** is:\n",
    "\n",
    "- to load the firsts two Hu moments for the images from each class which, as commented, was computed in previous notebook (you can use [`np.load()`](https://numpy.org/doc/stable/reference/generated/numpy.load.html?highlight=load#numpy.load)). \n",
    "- Then, **compute the mean** (or centroid) and **covariance matrix** for each class. You can compute the covariance matrix of a set of points using [`np.cov()`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1a\n",
    "#\n",
    "# Load first 2 Hu moments of each class \n",
    "train_triangles = np.load(\"./data/hu_triangles.npy\")[:,:2].T\n",
    "train_circles = np.load(\"./data/hu_circles.npy\")[:,:2].T\n",
    "train_squares = np.load(\"./data/hu_squares.npy\")[:,:2].T\n",
    "\n",
    "# Compute covariance matrices\n",
    "cov_triangles = np.cov(None)\n",
    "cov_circles = np.cov(None)\n",
    "cov_squares = np.cov(None)\n",
    "\n",
    "# Compute means\n",
    "mean_triangles = None\n",
    "mean_circles = None\n",
    "mean_squares = None\n",
    "\n",
    "print ('cov_triangles = \\n' + str(cov_triangles))\n",
    "print ('cov_circles = \\n' + str(cov_circles))\n",
    "print ('cov_squares = \\n' + str(cov_squares))\n",
    "print ('mean_triangles = ' + str(mean_triangles))\n",
    "print ('mean_circles = ' + str(mean_circles))\n",
    "print ('mean_squares = ' + str(mean_squares))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;\">Expected output:</span>\n",
    "```\n",
    "cov_triangles = \n",
    "[[1.17603861e-06 2.24894396e-07]\n",
    " [2.24894396e-07 8.80708956e-08]]\n",
    "cov_circles = \n",
    "[[1.02189926e-07 3.33231869e-08]\n",
    " [3.33231869e-08 1.09259042e-08]]\n",
    "cov_squares = \n",
    "[[1.61508686e-06 5.21788207e-07]\n",
    " [5.21788207e-07 1.70838002e-07]]\n",
    "mean_triangles = [0.19254439 0.00034407]\n",
    "mean_circles = [1.59537533e-01 9.98702969e-05]\n",
    "mean_squares = [0.16720802 0.00026462]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1b: Defining the discriminant function</i></b></span>**\n",
    "\n",
    "Your **next task** is to develop a method, named `discriminant_function()`, that computes the discriminant function for each class $d_k(x)$. The inputs have to be:\n",
    "\n",
    "- `features`: feature vector of dimension n (number of features, 2 in our problem).\n",
    "- `mu`: mean vector of the class k.\n",
    "- `cov`: covariance matrix with shape (n,n) of the class k.\n",
    "- `prior`: prior probability of class k.\n",
    "\n",
    "The method should evaluate (then return) the discriminant function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1b\n",
    "#\n",
    "def discriminant_function(features, mu, cov, prior):\n",
    "    \"\"\" Evaluates the discriminant function d(x)\n",
    "    \n",
    "        Args:\n",
    "            features: feature vector of dimension n\n",
    "            mu: mean vector of the class of which is being computed the probability\n",
    "            cov: covariance matrix with shape (n,n) of the class\n",
    "            prior: prior probability of class k\n",
    "\n",
    "        Returns:\n",
    "            dx: result of discriminant function\n",
    "    \"\"\"\n",
    "    covinv = np.linalg.inv(cov) # Auxiliar variable\n",
    "    dx = None # You can divide this computation in as many lines as you need\n",
    "    \n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can try** your function with the following test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([0.5, 0.6])\n",
    "mu = np.array([0.7, 0.9])\n",
    "cov = np.array([[0.7, 0.3],[0.3, 0.9]])\n",
    "prior = 0.5\n",
    "d = discriminant_function(f, mu, cov, prior)\n",
    "\n",
    "print ('d = ' + str(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;\">Expected output:</span>\n",
    "```\n",
    "d = -0.4433874441813701\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1c: Testing the classifier</i></b></span>**\n",
    "\n",
    "For testing our brand new classifier, we are going to classify some new images and check the results. *Note that the discriminant function is the logarithm of a probability, not a probability itself (values can be positive and negatives, but the result of the max function is the same)*.\n",
    "\n",
    "**What to do?** \n",
    "- Complete the auxiliary function `classify_image()` that:\n",
    "    1. computes the Hu moments of a testing image `sign_image`, and   \n",
    "    2. uses the discriminant function of each class to retrieve the highest output value. The class returning such a value would be the assigned one! $\\\\[10pt]$\n",
    "- After completing such a function, in the code cell below it, call it for the `test_circle.png`, `test_square.png` and `test_triangle.png` images in order to classify them. \n",
    "\n",
    "*We assume that there is no prior information about any class, so $P(C_i) = P(C_j)\\; \\forall i,j$. This can be interpreted as: while driving, we see the same number of circle, square and triangle shaped road signs.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1c\n",
    "#\n",
    "def image_moments(region):\n",
    "    \"\"\" Compute moments of the external contour in a binary image.   \n",
    "    \n",
    "        Args:\n",
    "            region: Binary image\n",
    "                    \n",
    "        Returns: \n",
    "            moments: dictionary containing all moments of the region\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Get external contour\n",
    "    contours,_ = cv2.findContours(region,cv2.RETR_EXTERNAL ,cv2.CHAIN_APPROX_NONE)\n",
    "    cnt = contours[0]\n",
    "    \n",
    "    # Compute moments\n",
    "    moments = cv2.moments(cnt)\n",
    "    \n",
    "    return moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(sign_image):\n",
    "    \"\"\" Classify a traffic sign image by its shape using a bayesian classifier   \n",
    "    \n",
    "        Args:\n",
    "            sign_image: Binarized image\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Compute Hu moments\n",
    "    moments = image_moments(sign_image)\n",
    "    hu = cv2.HuMoments(None).flatten()[None:None]\n",
    "    \n",
    "    # Classify circle test image\n",
    "    prior = 1/3\n",
    "    triangle = discriminant_function(None,None,None,None)\n",
    "    circle = discriminant_function(None,None,None,None)\n",
    "    square = discriminant_function(None,None,None,None)\n",
    "    \n",
    "    # Search the maximum\n",
    "    classification = max([None,None,None])\n",
    "    \n",
    "    if classification == triangle:\n",
    "        print(\"The sign is a triangle\\n\")\n",
    "    elif classification == circle:\n",
    "        print(\"The sign is a circle\\n\")\n",
    "    else:\n",
    "        print(\"The sign is a square\\n\")        \n",
    "    \n",
    "    return hu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "test_circle = cv2.imread(images_path + \"test_circle.png\", 0)\n",
    "test_triangle = cv2.imread(images_path + \"test_triangle.png\", 0)\n",
    "test_square = cv2.imread(images_path + \"test_square.png\", 0)\n",
    "\n",
    "# Classify them\n",
    "print(\"Circle: \")\n",
    "moments_circle = classify_image(None)\n",
    "print(\"Triangle: \")\n",
    "moments_triangle = classify_image(None)\n",
    "print(\"Square: \")\n",
    "moments_square = classify_image(None)\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots()\n",
    "plt.axis([0.158, 0.197, -0.02, 0.02])\n",
    "\n",
    "# Plot hu moments\n",
    "plt.plot(train_triangles[0,:],train_triangles[1,:],'go')\n",
    "plt.plot(train_circles[0,:],train_circles[1,:],'ro')\n",
    "plt.plot(train_squares[0,:],train_squares[1,:],'bo')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('x_2')\n",
    "\n",
    "# Plot testing data\n",
    "plt.plot(moments_circle[0],moments_circle[1],'*m',markersize=12)\n",
    "plt.plot(moments_triangle[0],moments_triangle[1],'*m',markersize=12)\n",
    "plt.plot(moments_square[0],moments_square[1],'*m',markersize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1d: Analyzing covariances</i></b></span>**\n",
    "\n",
    "Finally, we can see how this classifier divides the feature space showing the computed covariance ellipses. **You have to** complete the following code cell to make it works, showing the covariance ellipses of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax = plt.subplots()\n",
    "plt.axis([0.14, 0.2, -0.03, 0.03])\n",
    "\n",
    "# Plot hu moments\n",
    "plt.plot(train_triangles[0,:],train_triangles[1,:],'go')\n",
    "plt.plot(train_circles[0,:],train_circles[1,:],'ro')\n",
    "plt.plot(train_squares[0,:],train_squares[1,:],'bo')\n",
    "\n",
    "# Plot ellipses representing covariance matrices\n",
    "PlotEllipse(fig, ax, np.vstack(None), None, 15, color='black')\n",
    "PlotEllipse(fig, ax, np.vstack(None), None, 15, color='black')\n",
    "PlotEllipse(fig, ax, np.vstack(None), None, 15, color='black')\n",
    "\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplification of the Naïve classifier\n",
    "\n",
    "The classifier at hand can be simplified if the Euclidean distance is considered instead of the Mahalanobis one. This can be achieved using isotropic covariance matrices:\n",
    "\n",
    "\n",
    "$$\\Sigma^k = \\Sigma = \\sigma^2 \\cdot I = \\sigma^2 \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "In this way, decision boundaries are lines, and covariances are spherical. This is called a **natural classifier**:\n",
    "\n",
    "<img src=\"./images/natural_classifier.png\" width=\"500\"><center><i>Example of feature space with 3 classes characterized by Gaussian distributions with isotropic covariances. Black lines are decision boundaries.</i></center>$\\\\[3pt]$\n",
    "\n",
    "In this case, the discriminant function can be simplified, and the quadratic term disappears:$\\\\[6pt]$\n",
    "\n",
    "$$d_k(x) = -(\\mathbf{x-\\mu^k})^T(\\mathbf{x-\\mu^k}) = -||\\mathbf{x-\\mu^k}||^2\\\\[6pt]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Playing with isotropic covariance matrices</i></b></span>**\n",
    "\n",
    "\n",
    "**What to do?** Repeat the previous steps but using isotropic covariance matrices. Recall that [`np.eye()`](https://numpy.org/doc/stable/reference/generated/numpy.eye.html) defines an identity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2\n",
    "#\n",
    "def discriminant_function_isotropic(features, mu):\n",
    "    \"\"\" Evaluates the discriminant function of a naive Bayes clasifier using isotropic covariances\n",
    "    \n",
    "        Args:\n",
    "            features: feature vector of dimension n\n",
    "            mu: mean vector of the class of which is being computed the probability\n",
    "\n",
    "        Returns:\n",
    "            dx: result of discriminant function\n",
    "    \"\"\"\n",
    "    dx = None\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image_isotropic(sign_image):\n",
    "    \"\"\" Classify a traffic sign image by its shape using a bayesian classifier   \n",
    "    \n",
    "        Args:\n",
    "            sign_image: Binarized image\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Compute Hu moments\n",
    "    moments = image_moments(sign_image)\n",
    "    hu = cv2.HuMoments(None).flatten()[None:None]\n",
    "    \n",
    "    # Classify circle test image\n",
    "    triangle = discriminant_function_isotropic(None,None)\n",
    "    circle = discriminant_function_isotropic(None,None)\n",
    "    square = discriminant_function_isotropic(None,None)\n",
    "    \n",
    "    # Search the maximum\n",
    "    classification = max([None,None,None])\n",
    "    \n",
    "    if classification == triangle:\n",
    "        print(\"The sign is a triangle\\n\")\n",
    "    elif classification == circle:\n",
    "        print(\"The sign is a circle\\n\")\n",
    "    else:\n",
    "        print(\"The sign is a square\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "test_circle = cv2.imread(images_path + \"test_circle.png\", 0)\n",
    "test_triangle = cv2.imread(images_path + \"test_triangle.png\", 0)\n",
    "test_square = cv2.imread(images_path + \"test_square.png\", 0)\n",
    "\n",
    "# Classify them\n",
    "print(\"Circle: \")\n",
    "classify_image_isotropic(None)\n",
    "print(\"Triangle: \")\n",
    "classify_image_isotropic(None)\n",
    "print(\"Square: \")\n",
    "classify_image_isotropic(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the following code** to observe the isotropic covariance matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax = plt.subplots()\n",
    "plt.axis([0.14, 0.2, -0.03, 0.03])\n",
    "\n",
    "# Plot hu moments\n",
    "plt.plot(train_triangles[0,:],train_triangles[1,:],'go')\n",
    "plt.plot(train_circles[0,:],train_circles[1,:],'ro')\n",
    "plt.plot(train_squares[0,:],train_squares[1,:],'bo')\n",
    "\n",
    "# Plot ellipses representing covariance matrices\n",
    "PlotEllipse(fig, ax, np.vstack(None), None, 0.003, color='black')\n",
    "PlotEllipse(fig, ax, np.vstack(None), None, 0.003, color='black')\n",
    "PlotEllipse(fig, ax, np.vstack(None), None, 0.003, color='black')\n",
    "\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n",
    "\n",
    "Now that you are an expert concerning the Naïve Bayesian classifier, **answer the following questions**:\n",
    "\n",
    "- Considering the classifier that you implemented in assignment 1c, to which class would be assigned an object with `x_1=0.17` and `x_2=-0.01`? And considering the one in assignment 2?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "- What are the pros and cons of using isotropic covariances?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- In what type of problems could isotropic matrices be used?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- In your opinion, is it worth to consider a Bayes classifier when dealing with this problem? In which situations could this classifier show its potential?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Awesome! You now know how to design a classifier for previously segmented and characterized objects. Note that for more complex shapes, you can use the **7 Hu moments instead of the two that we used**. We reduced their number just for visualization and simplicity purposes.\n",
    "\n",
    "In this notebook you have learned to:\n",
    "\n",
    "- construct a Naïve Bayesian classifier and apply it to a real problem where features follow a normal distribution,\n",
    "- build a simplified classifier where isotropic covariances are assumed, and\n",
    "- improve a classifier (if needed) using rejection regions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
