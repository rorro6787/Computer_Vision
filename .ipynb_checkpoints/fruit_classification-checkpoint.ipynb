{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zet_QwCbHtZs"
   },
   "source": [
    "# **Convolutional Neural Networks for Image Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktqLUyvJHtZt"
   },
   "source": [
    "## **Introduction**\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have transformed image classification by effectively capturing spatial patterns in data. Inspired by the visual cortex, CNNs excel at recognizing features and generalizing them across images. This work examines their application in image classification, highlighting their structure and functionality.\n",
    "\n",
    "The first step when dealing with a deep learning project is to ensure we have access to GPU acceleration, as it significantly speeds up training and computation, especially for large datasets and complex models. This can be verified by checking the system's hardware capabilities and configuring the deep learning framework (such as TensorFlow or PyTorch) to utilize the GPU. If a GPU is not available locally, cloud platforms like Google Colab, AWS, or Azure can provide GPU instances for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Oc7NKS6I91L",
    "outputId": "5ad382dd-21de-4ba7-8d69-f06024ad33fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 11 11:04:38 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   73C    P0              33W /  70W |  11813MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OltRojKpHtZu",
    "outputId": "4e64a8f8-a346-430d-ea66-a7b78c56ee95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used to create block diagrams\n",
    "from graphviz import Digraph\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.io as tv_io\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import triton\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqLw7EAXHtZw"
   },
   "source": [
    "## The Convolutional Deep Learning Model\n",
    "\n",
    "We will use as starting point a pretrained model which a deep learning model that has already been trained on a large dataset (e.g., ImageNet). It captures general features like edges, shapes, and textures, which can be reused for specific tasks through transfer learning.\n",
    "\n",
    "### Benefits of Using a Pretrained Model\n",
    "- **Transfer Learning**: Leverages general features for specific tasks.\n",
    "- **Reduced Training Time**: Avoids retraining basic patterns.\n",
    "- **Better Performance**: Works well even with limited data.\n",
    "- **Expert Design**: Uses models trained by experts on high-quality datasets.\n",
    "\n",
    "### Why Use `vgg16` with `VGG16_Weights.DEFAULT`?\n",
    "- **VGG16**: A deep and effective CNN architecture for image classification.\n",
    "- **Pretrained Weights**: Trained on ImageNet, providing a strong foundation for feature extraction.\n",
    "- **Advantages**: Saves time, boosts performance, and simplifies fine-tuning for your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "pIzlGugiJ0qE"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "vgg_model = vgg16(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iZhwPjuKEuy",
    "outputId": "6b451d67-5a86-455c-fc38-fdf47b83a7f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "pEbCiGEKMYh7",
    "outputId": "3943a65e-f071-45f4-9f62-d89cd3645aa1"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"354pt\" height=\"702pt\"\n",
       " viewBox=\"0.00 0.00 354.28 702.05\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 698.05)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-698.05 350.28,-698.05 350.28,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"338.9,-694.05 75.21,-694.05 7.38,-618.05 271.07,-618.05 338.9,-694.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-659.85\" font-family=\"Times,serif\" font-size=\"14.00\">2 convolutional layers</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-644.85\" font-family=\"Times,serif\" font-size=\"14.00\">(64 filters) + max&#45;pooling</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>B</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"346.41,-582.05 70.77,-582.05 -0.14,-506.05 275.5,-506.05 346.41,-582.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-547.85\" font-family=\"Times,serif\" font-size=\"14.00\">2 convolutional layers</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-532.85\" font-family=\"Times,serif\" font-size=\"14.00\">(128 filters) + max&#45;pooling</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"green\" stroke-width=\"2\" d=\"M173.14,-617.81C173.14,-609.66 173.14,-600.92 173.14,-592.43\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" stroke-width=\"2\" points=\"176.64,-592.26 173.14,-582.26 169.64,-592.26 176.64,-592.26\"/>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>C</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"346.41,-470.05 70.77,-470.05 -0.14,-394.05 275.5,-394.05 346.41,-470.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-435.85\" font-family=\"Times,serif\" font-size=\"14.00\">3 convolutional layers</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-420.85\" font-family=\"Times,serif\" font-size=\"14.00\">(256 filters) + max&#45;pooling</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;C -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>B&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"green\" stroke-width=\"2\" stroke-dasharray=\"5,2\" d=\"M173.14,-505.81C173.14,-497.66 173.14,-488.92 173.14,-480.43\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" stroke-width=\"2\" points=\"176.64,-480.26 173.14,-470.26 169.64,-480.26 176.64,-480.26\"/>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>D</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"346.41,-358.05 70.77,-358.05 -0.14,-282.05 275.5,-282.05 346.41,-358.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-323.85\" font-family=\"Times,serif\" font-size=\"14.00\">3 convolutional layers</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-308.85\" font-family=\"Times,serif\" font-size=\"14.00\">(512 filters) + max&#45;pooling</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;D -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>C&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"green\" stroke-width=\"2\" stroke-dasharray=\"5,2\" d=\"M173.14,-393.81C173.14,-385.66 173.14,-376.92 173.14,-368.43\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" stroke-width=\"2\" points=\"176.64,-368.26 173.14,-358.26 169.64,-368.26 176.64,-368.26\"/>\n",
       "</g>\n",
       "<!-- E -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>E</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"346.41,-246.05 70.77,-246.05 -0.14,-170.05 275.5,-170.05 346.41,-246.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-211.85\" font-family=\"Times,serif\" font-size=\"14.00\">3 convolutional layers</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-196.85\" font-family=\"Times,serif\" font-size=\"14.00\">(512 filters) + max&#45;pooling</text>\n",
       "</g>\n",
       "<!-- D&#45;&gt;E -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>D&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"green\" stroke-width=\"2\" stroke-dasharray=\"5,2\" d=\"M173.14,-281.81C173.14,-273.66 173.14,-264.92 173.14,-256.43\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" stroke-width=\"2\" points=\"176.64,-256.26 173.14,-246.26 169.64,-256.26 176.64,-256.26\"/>\n",
       "</g>\n",
       "<!-- F -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>F</title>\n",
       "<polygon fill=\"orange\" stroke=\"orange\" points=\"266.3,-103.03 219.72,-134.08 126.56,-134.08 79.98,-103.03 126.56,-71.97 219.72,-71.97 266.3,-103.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-106.83\" font-family=\"Times,serif\" font-size=\"14.00\">Adaptive Average</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-91.83\" font-family=\"Times,serif\" font-size=\"14.00\">Pooling (7x7)</text>\n",
       "</g>\n",
       "<!-- E&#45;&gt;F -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>E&#45;&gt;F</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"2\" d=\"M173.14,-169.86C173.14,-161.5 173.14,-152.59 173.14,-144.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"2\" points=\"176.64,-144.07 173.14,-134.07 169.64,-144.07 176.64,-144.07\"/>\n",
       "</g>\n",
       "<!-- G -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"241.14,-36 105.14,-36 105.14,0 241.14,0 241.14,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">Fully connected MLP</text>\n",
       "</g>\n",
       "<!-- F&#45;&gt;G -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>F&#45;&gt;G</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-width=\"2\" d=\"M173.14,-71.58C173.14,-63.34 173.14,-54.48 173.14,-46.42\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" stroke-width=\"2\" points=\"176.64,-46.41 173.14,-36.41 169.64,-46.41 176.64,-46.41\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x727122109940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = Digraph()\n",
    "\n",
    "# Personalización de nodos\n",
    "dot.node('A', '2 convolutional layers\\n(64 filters) + max-pooling', shape='parallelogram', style='filled', color='lightblue', fontcolor='black')\n",
    "dot.node('B', '2 convolutional layers\\n(128 filters) + max-pooling', shape='parallelogram', style='filled', color='lightblue', fontcolor='black')\n",
    "dot.node('C', '3 convolutional layers\\n(256 filters) + max-pooling', shape='parallelogram', style='filled', color='lightblue', fontcolor='black')\n",
    "dot.node('D', '3 convolutional layers\\n(512 filters) + max-pooling', shape='parallelogram', style='filled', color='lightblue', fontcolor='black')\n",
    "dot.node('E', '3 convolutional layers\\n(512 filters) + max-pooling', shape='parallelogram', style='filled', color='lightblue', fontcolor='black')\n",
    "dot.node('F', 'Adaptive Average\\nPooling (7x7)', shape='hexagon', style='filled', color='orange', fontcolor='black')\n",
    "dot.node('G', 'Fully connected MLP', shape='rect', style='filled', color='red', fontcolor='white')\n",
    "\n",
    "# Personalización de bordes\n",
    "dot.edge('A', 'B', color='green', penwidth='2.0')\n",
    "dot.edge('B', 'C', color='green', penwidth='2.0', style='dashed')\n",
    "dot.edge('C', 'D', color='green', penwidth='2.0', style='dashed')\n",
    "dot.edge('D', 'E', color='green', penwidth='2.0', style='dashed')\n",
    "dot.edge('E', 'F', color='black', penwidth='2.0', style='bold')\n",
    "dot.edge('F', 'G', color='blue', penwidth='2.0', style='bold')\n",
    "\n",
    "# Renderizar\n",
    "dot.render('simple_diagram', format='svg', cleanup=True)\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsDVyYmdPEl6"
   },
   "source": [
    "## Adapting a Pretrained Model for our Problem\n",
    "\n",
    "While a pretrained model provides a solid starting point by leveraging learned features from a large dataset like ImageNet, it is important to adapt it for your specific problem. This is because the pretrained model is generally designed for a different task (e.g., ImageNet classification with 1,000 classes) than what you are working on (e.g., classifying images into 6 classes).\n",
    "\n",
    "In this project, you're training a machine learning model to classify fruits as fresh or rotten. The dataset you're using, which is organized in the `data/fruits` folder, includes six categories:\n",
    "\n",
    "- Fresh Apples\n",
    "- Fresh Oranges\n",
    "- Fresh Bananas\n",
    "- Rotten Apples\n",
    "- Rotten Oranges\n",
    "- Rotten Bananas\n",
    "\n",
    "The task at hand is a **multi-class classification** problem, where the model needs to assign an image to one of the six categories, therefore we build this new model from the pretrained one, therefore, the first step is to freeze the pretrain model's weights. This is important because the convolutional layers in the pretrained VGG16 model have already learned general features like edges, shapes, and textures. Freezing these layers prevents them from being overwritten during training, ensuring that this valuable knowledge is retained, plus it provides a much faster training process and avoidance of overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CD1Xj8k9MYmw",
    "outputId": "91cc39d2-cd30-4142-c871-ce6fd77239f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze base model\n",
    "vgg_model.requires_grad_(False)\n",
    "next(iter(vgg_model.parameters())).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjydPFKGMYoy",
    "outputId": "e6560f34-1713-4320-9126-fec28db2d817"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.classifier[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sD_nPPuFKE41",
    "outputId": "d5a05903-e405-4859-fd22-1eb8b74d01fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (4): Linear(in_features=4096, out_features=500, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_CLASSES = 6\n",
    "\n",
    "my_model = nn.Sequential(\n",
    "    vgg_model.features,\n",
    "    vgg_model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    vgg_model.classifier[0:3],\n",
    "    nn.Linear(4096, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, N_CLASSES)\n",
    ")\n",
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "apZegSHaQCTi",
    "outputId": "ac123a61-1864-41ec-ac31-042f45619c33"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"354pt\" height=\"774pt\"\n",
       " viewBox=\"0.00 0.00 354.28 774.05\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 770.05)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-770.05 350.28,-770.05 350.28,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"338.9,-766.05 75.21,-766.05 7.38,-690.05 271.07,-690.05 338.9,-766.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-731.85\" font-family=\"Times,serif\" font-size=\"14.00\">2 convolutional layers</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-716.85\" font-family=\"Times,serif\" font-size=\"14.00\">(64 filters) + max&#45;pooling</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>B</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"346.41,-654.05 70.77,-654.05 -0.14,-578.05 275.5,-578.05 346.41,-654.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-619.85\" font-family=\"Times,serif\" font-size=\"14.00\">2 convolutional layers</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-604.85\" font-family=\"Times,serif\" font-size=\"14.00\">(128 filters) + max&#45;pooling</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"green\" stroke-width=\"2\" d=\"M173.14,-689.81C173.14,-681.66 173.14,-672.92 173.14,-664.43\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" stroke-width=\"2\" points=\"176.64,-664.26 173.14,-654.26 169.64,-664.26 176.64,-664.26\"/>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>C</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"346.41,-542.05 70.77,-542.05 -0.14,-466.05 275.5,-466.05 346.41,-542.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-507.85\" font-family=\"Times,serif\" font-size=\"14.00\">3 convolutional layers</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-492.85\" font-family=\"Times,serif\" font-size=\"14.00\">(256 filters) + max&#45;pooling</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;C -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>B&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"green\" stroke-width=\"2\" stroke-dasharray=\"5,2\" d=\"M173.14,-577.81C173.14,-569.66 173.14,-560.92 173.14,-552.43\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" stroke-width=\"2\" points=\"176.64,-552.26 173.14,-542.26 169.64,-552.26 176.64,-552.26\"/>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>D</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"346.41,-430.05 70.77,-430.05 -0.14,-354.05 275.5,-354.05 346.41,-430.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-395.85\" font-family=\"Times,serif\" font-size=\"14.00\">3 convolutional layers</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-380.85\" font-family=\"Times,serif\" font-size=\"14.00\">(512 filters) + max&#45;pooling</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;D -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>C&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"green\" stroke-width=\"2\" stroke-dasharray=\"5,2\" d=\"M173.14,-465.81C173.14,-457.66 173.14,-448.92 173.14,-440.43\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" stroke-width=\"2\" points=\"176.64,-440.26 173.14,-430.26 169.64,-440.26 176.64,-440.26\"/>\n",
       "</g>\n",
       "<!-- E -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>E</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"346.41,-318.05 70.77,-318.05 -0.14,-242.05 275.5,-242.05 346.41,-318.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-283.85\" font-family=\"Times,serif\" font-size=\"14.00\">3 convolutional layers</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-268.85\" font-family=\"Times,serif\" font-size=\"14.00\">(512 filters) + max&#45;pooling</text>\n",
       "</g>\n",
       "<!-- D&#45;&gt;E -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>D&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"green\" stroke-width=\"2\" stroke-dasharray=\"5,2\" d=\"M173.14,-353.81C173.14,-345.66 173.14,-336.92 173.14,-328.43\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" stroke-width=\"2\" points=\"176.64,-328.26 173.14,-318.26 169.64,-328.26 176.64,-328.26\"/>\n",
       "</g>\n",
       "<!-- F -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>F</title>\n",
       "<polygon fill=\"orange\" stroke=\"orange\" points=\"266.3,-175.03 219.72,-206.08 126.56,-206.08 79.98,-175.03 126.56,-143.97 219.72,-143.97 266.3,-175.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-178.83\" font-family=\"Times,serif\" font-size=\"14.00\">Adaptive Average</text>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-163.83\" font-family=\"Times,serif\" font-size=\"14.00\">Pooling (7x7)</text>\n",
       "</g>\n",
       "<!-- E&#45;&gt;F -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>E&#45;&gt;F</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-width=\"2\" d=\"M173.14,-241.86C173.14,-233.5 173.14,-224.59 173.14,-216.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" stroke-width=\"2\" points=\"176.64,-216.07 173.14,-206.07 169.64,-216.07 176.64,-216.07\"/>\n",
       "</g>\n",
       "<!-- G -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>G</title>\n",
       "<ellipse fill=\"green\" stroke=\"green\" cx=\"173.14\" cy=\"-90\" rx=\"68.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">Flatten Operator</text>\n",
       "</g>\n",
       "<!-- F&#45;&gt;G -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>F&#45;&gt;G</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-width=\"2\" d=\"M173.14,-143.58C173.14,-135.34 173.14,-126.48 173.14,-118.42\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" stroke-width=\"2\" points=\"176.64,-118.41 173.14,-108.41 169.64,-118.41 176.64,-118.41\"/>\n",
       "</g>\n",
       "<!-- H -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>H</title>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"268.64,-36 77.64,-36 77.64,0 268.64,0 268.64,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.14\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">Extended Fully connected MLP</text>\n",
       "</g>\n",
       "<!-- G&#45;&gt;H -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>G&#45;&gt;H</title>\n",
       "<path fill=\"none\" stroke=\"orange\" stroke-width=\"2\" d=\"M173.14,-71.7C173.14,-63.98 173.14,-54.71 173.14,-46.11\"/>\n",
       "<polygon fill=\"orange\" stroke=\"orange\" stroke-width=\"2\" points=\"176.64,-46.1 173.14,-36.1 169.64,-46.1 176.64,-46.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x727122be7d10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = Digraph()\n",
    "\n",
    "# Personalización de nodos\n",
    "dot.node('A', '2 convolutional layers\\n(64 filters) + max-pooling', shape='parallelogram', style='filled', color='lightblue', fontcolor='black')\n",
    "dot.node('B', '2 convolutional layers\\n(128 filters) + max-pooling', shape='parallelogram', style='filled', color='lightblue', fontcolor='black')\n",
    "dot.node('C', '3 convolutional layers\\n(256 filters) + max-pooling', shape='parallelogram', style='filled', color='lightblue', fontcolor='black')\n",
    "dot.node('D', '3 convolutional layers\\n(512 filters) + max-pooling', shape='parallelogram', style='filled', color='lightblue', fontcolor='black')\n",
    "dot.node('E', '3 convolutional layers\\n(512 filters) + max-pooling', shape='parallelogram', style='filled', color='lightblue', fontcolor='black')\n",
    "dot.node('F', 'Adaptive Average\\nPooling (7x7)', shape='hexagon', style='filled', color='orange', fontcolor='black')\n",
    "dot.node('G', 'Flatten Operator', shape='ellipse', style='filled', color='green', fontcolor='white')\n",
    "dot.node('H', 'Extended Fully connected MLP', shape='rect', style='filled', color='red', fontcolor='white')\n",
    "\n",
    "# Personalización de bordes\n",
    "dot.edge('A', 'B', color='green', penwidth='2.0')\n",
    "dot.edge('B', 'C', color='green', penwidth='2.0', style='dashed')\n",
    "dot.edge('C', 'D', color='green', penwidth='2.0', style='dashed')\n",
    "dot.edge('D', 'E', color='green', penwidth='2.0', style='dashed')\n",
    "dot.edge('E', 'F', color='black', penwidth='2.0', style='bold')\n",
    "dot.edge('F', 'G', color='blue', penwidth='2.0', style='bold')\n",
    "dot.edge('G', 'H', color='orange', penwidth='2.0', style='bold')\n",
    "\n",
    "# Renderizar\n",
    "dot.render('simple_diagram', format='svg', cleanup=True)\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpMCWcTmRRdT"
   },
   "source": [
    "## Compiling the Model\n",
    "\n",
    "The next step, once we have the architecture of the model prepared is to compile it. This is an optional step but very recommened because in pytorch to compile the model optimizes the model by applying various transformations to improve performance during training and inference. These optimizations can include better memory management, faster execution on hardware accelerators (like GPUs), and more efficient computation graphs.\n",
    "\n",
    "### Understanding Entropies in Machine Learning Loss Functions\n",
    "\n",
    "Entropy is a key concept in information theory and machine learning, representing uncertainty or unpredictability in data. Different entropy-based loss functions are used depending on the type of task: **binary classification**, **multi-class classification**, or **multi-label classification**. Below is an explanation of the common entropies and when to use them.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Binary Cross-Entropy (Log Loss)\n",
    "**Use Case:** Binary classification problems (e.g., \"cat vs. dog\").\n",
    "\n",
    "##### Explanation\n",
    "Binary cross-entropy measures the difference between the true label (0 or 1) and the predicted probability for a single output neuron. It evaluates how well the model’s predicted probability aligns with the actual class.\n",
    "\n",
    "##### Formula\n",
    "$$\n",
    "\\text{Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "y_i: \\text{Actual label (0 or 1).}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}_i: \\text{Predicted probability for class 1.}\n",
    "$$\n",
    "\n",
    "$$\n",
    "N: \\text{Total number of samples.}\n",
    "$$\n",
    "\n",
    "\n",
    "##### Key Points\n",
    "- Requires a **single output neuron** with a **sigmoid activation function**.\n",
    "- Outputs a probability value between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Categorical Cross-Entropy\n",
    "**Use Case:** Multi-class classification problems (e.g., \"dog vs. cat vs. bird\").\n",
    "\n",
    "##### Explanation\n",
    "Categorical cross-entropy measures the difference between the true class (one-hot encoded) and the predicted probabilities across all classes. It evaluates how well the predicted probability distribution matches the true distribution.\n",
    "\n",
    "##### Formula\n",
    "$$\n",
    "\\text{Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\cdot \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "$$\n",
    "y_{ij}: \\text{Actual label for class } j \\text{ (1 if correct, 0 otherwise).}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}_{ij}: \\text{Predicted probability for class } j.\n",
    "$$\n",
    "\n",
    "$$\n",
    "N: \\text{Total number of samples.}\n",
    "$$\n",
    "\n",
    "$$\n",
    "C: \\text{Total number of classes.}\n",
    "$$\n",
    "\n",
    "##### Key Points\n",
    "- Requires **one output neuron per class** with a **softmax activation function**.\n",
    "- The predicted outputs are normalized probabilities (sum to 1).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Sparse Categorical Cross-Entropy\n",
    "**Use Case:** Multi-class classification with integer-encoded labels.\n",
    "\n",
    "##### Explanation\n",
    "Sparse categorical cross-entropy is similar to categorical cross-entropy but works with integer-encoded labels instead of one-hot encoding. This makes it memory efficient when dealing with a large number of classes.\n",
    "\n",
    "##### Key Points\n",
    "- Suitable for **multi-class classification** with **large class counts**.\n",
    "- Requires a **softmax activation function**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Binary Cross-Entropy for Multi-Label Classification\n",
    "**Use Case:** Multi-label classification problems (e.g., predicting multiple diseases).\n",
    "\n",
    "##### Explanation\n",
    "Binary cross-entropy is extended for tasks where each example can belong to multiple classes. Each class is treated independently as a binary classification problem.\n",
    "\n",
    "##### Key Points\n",
    "- Requires **one output neuron per label** with a **sigmoid activation function**.\n",
    "- Outputs independent probabilities for each label.\n",
    "\n",
    "---\n",
    "\n",
    "By choosing the appropriate entropy-based loss function for your task, you ensure the model learns effectively and achieves optimal results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "_fok_JKCQJKl"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(my_model.parameters())\n",
    "my_model = torch.compile(my_model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsBglgKbRsSy"
   },
   "source": [
    "## Data Augmentation\n",
    "Data augmentation is a technique used to artificially increase the size of a training dataset by applying random transformations to the original images. This helps the model generalize better and avoid overfitting, especially when you have limited data. In your case, the random transformations such as flips, rotations, and color adjustments allow the model to learn more robust features from the images of fruits.\n",
    "\n",
    "#### Key Reasons for Using Data Augmentation:\n",
    "\n",
    "1. **Increased Data Variability**: By applying transformations like random rotation, horizontal/vertical flips, and color changes, the model is exposed to more varied versions of the same image. This helps the model learn to identify the object (e.g., fruits) under different conditions and perspectives.\n",
    "\n",
    "2. **Prevention of Overfitting**: When the model sees the same image multiple times, it might memorize the details rather than generalize. Data augmentation introduces randomness, which helps the model focus on learning the general features of the images rather than memorizing specific patterns.\n",
    "\n",
    "3. **Better Generalization**: Augmentation encourages the model to become invariant to certain transformations (like rotation or color variation). This means the model will be better at recognizing fruits in real-world scenarios, where the appearance may vary (e.g., an orange could be rotated or appear brighter due to lighting).\n",
    "\n",
    "4. **Improved Robustness**: The model becomes more robust to noise and variations in the data, which is important when deployed in real-world situations where input images can vary widely.\n",
    "\n",
    "#### Example Transformations Applied:\n",
    "\n",
    "- **Random Horizontal Flip**: Helps the model recognize fruits regardless of their orientation.\n",
    "- **Random Rotation**: Teaches the model to recognize fruits from different angles.\n",
    "- **Color Jitter**: Introduces variations in brightness, contrast, and color, helping the model adapt to different lighting conditions.\n",
    "- **Random Resized Crop**: Provides different zoom levels of the fruit, improving recognition at varying distances.\n",
    "- **Normalization**: Ensures the pixel values are scaled similarly to how the pretrained VGG16 model was trained, which helps the model perform better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "7A3_uEJ-QJNA"
   },
   "outputs": [],
   "source": [
    "pre_trans = weights.transforms()\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
    "\n",
    "random_trans = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),                                                 # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(20),                                                     # Randomly rotate the image by up to 20 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),     # Random changes in color\n",
    "    transforms.RandomResizedCrop(IMG_WIDTH, scale=(0.8, 1.0)),                         # Crop a random portion and resize\n",
    "    transforms.RandomVerticalFlip(),                                                   # Randomly flip the image vertically\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])        # Normalize using ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS8tuZCCSZDy"
   },
   "source": [
    "## **The Dataset**\n",
    "In this project, I’m training a machine learning model to classify fruits as fresh or rotten. I’m using a dataset from [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification), which I’ve organized in the `data/fruits` folder.\n",
    "\n",
    "The dataset includes six categories:\n",
    "- Fresh Apples\n",
    "- Fresh Oranges\n",
    "- Fresh Bananas\n",
    "- Rotten Apples\n",
    "- Rotten Oranges\n",
    "- Rotten Bananas\n",
    "\n",
    "To tackle this multi-class classification problem:\n",
    "1. I’ve designed the model’s output layer with **6 neurons**, each representing one category.\n",
    "2. Since there are more than two classes, I’ve compiled the model using the `categorical_crossentropy` loss function to ensure it learns effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "kcvrFaSvQJPM"
   },
   "outputs": [],
   "source": [
    "DATA_LABELS = [\"freshapples\", \"freshbanana\", \"freshoranges\", \"rottenapples\", \"rottenbanana\", \"rottenoranges\"]\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for l_idx, label in enumerate(DATA_LABELS):\n",
    "            data_paths = glob.glob(data_dir + label + '/*.png', recursive=True)\n",
    "            for path in data_paths:\n",
    "                img = tv_io.read_image(path, tv_io.ImageReadMode.RGB)\n",
    "                self.imgs.append(pre_trans(img).to(device))\n",
    "                self.labels.append(torch.tensor(l_idx).to(device))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "MJ6Itx7kQJRt"
   },
   "outputs": [],
   "source": [
    "n = 32 # Example batch size, adjust based on memory and dataset size\n",
    "\n",
    "train_path = \"data/fruits/train/\"\n",
    "train_data = MyDataset(train_path)\n",
    "# Shuffle for training\n",
    "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_path = \"data/fruits/valid/\"\n",
    "valid_data = MyDataset(valid_path)\n",
    "# No shuffle for validation\n",
    "valid_loader = DataLoader(valid_data, batch_size=n, shuffle=False)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WirQ4ylOUY47"
   },
   "source": [
    "## Model Training\n",
    "Now that we've prepared the model and dataset, it's time to start training. The following loop runs for a specified number of epochs (in this case, 10) and iterates through the entire training dataset to adjust the model's weights.\n",
    "\n",
    "### Epochs and Iterations\n",
    "\n",
    "- **Epoch**: One full pass through the entire training dataset. Each epoch allows the model to adjust its weights and improve its performance.\n",
    "- **Train Step**: In the `train` function, the model will be trained on the training data (`train_loader`), and after each epoch, the `validate` function will check the model's performance on a separate validation set (`valid_loader`).\n",
    "\n",
    "By training for multiple epochs, the model will progressively improve and learn better features for classifying fruits, while data augmentation helps it to remain flexible and robust in real-world scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4IHpXdoCUIPM",
    "outputId": "3fef3cee-c53b-4c9e-cb06-fcef0c8986af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 31.0263 Accuracy: 0.6946\n",
      "Valid - Loss: 3.0541 Accuracy: 0.9058\n",
      "Epoch: 1\n",
      "Train - Loss: 18.3256 Accuracy: 0.8139\n",
      "Valid - Loss: 4.0150 Accuracy: 0.8845\n",
      "Epoch: 2\n",
      "Train - Loss: 16.3736 Accuracy: 0.8291\n",
      "Valid - Loss: 2.7707 Accuracy: 0.9179\n",
      "Epoch: 3\n",
      "Train - Loss: 16.7442 Accuracy: 0.8257\n",
      "Valid - Loss: 4.6469 Accuracy: 0.8815\n",
      "Epoch: 4\n",
      "Train - Loss: 12.3011 Accuracy: 0.8680\n",
      "Valid - Loss: 3.6142 Accuracy: 0.9149\n",
      "Epoch: 5\n",
      "Train - Loss: 12.4378 Accuracy: 0.8816\n",
      "Valid - Loss: 3.7662 Accuracy: 0.8906\n",
      "Epoch: 6\n",
      "Train - Loss: 13.2005 Accuracy: 0.8621\n",
      "Valid - Loss: 3.2573 Accuracy: 0.9149\n",
      "Epoch: 7\n",
      "Train - Loss: 13.5954 Accuracy: 0.8672\n",
      "Valid - Loss: 2.5707 Accuracy: 0.9210\n",
      "Epoch: 8\n",
      "Train - Loss: 14.5035 Accuracy: 0.8519\n",
      "Valid - Loss: 2.4404 Accuracy: 0.9301\n",
      "Epoch: 9\n",
      "Train - Loss: 11.3890 Accuracy: 0.8866\n",
      "Valid - Loss: 4.1041 Accuracy: 0.9119\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQVhpkSBWOkA"
   },
   "source": [
    "## Model Fine-Tunning (optinal)\n",
    "\n",
    "After the training process, we can already see a validation accuracy of over 85%, which is very good considering how computationally lightweight the process was. However, there is still one final step to further increase the accuracy. By unfreezing the pretrained model's weights and performing one last small training step with a very low learning rate, we can significantly boost its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "7oTOcD4AHtZ7"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "vgg_model.requires_grad_(True)\n",
    "optimizer = Adam(my_model.parameters(), lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RKmJPRPHtZ7",
    "outputId": "aa40fc17-c4d0-4960-910a-97d475fabdd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 21.0974 Accuracy: 0.8003\n",
      "Valid - Loss: 11.7380 Accuracy: 0.7690\n",
      "Epoch: 1\n",
      "Train - Loss: 14.2944 Accuracy: 0.8604\n",
      "Valid - Loss: 10.9251 Accuracy: 0.7325\n",
      "Epoch: 2\n",
      "Train - Loss: 11.6738 Accuracy: 0.9010\n",
      "Valid - Loss: 4.1618 Accuracy: 0.8632\n",
      "Epoch: 3\n",
      "Train - Loss: 8.7681 Accuracy: 0.9120\n",
      "Valid - Loss: 6.3436 Accuracy: 0.8511\n",
      "Epoch: 4\n",
      "Train - Loss: 10.0515 Accuracy: 0.9162\n",
      "Valid - Loss: 4.6340 Accuracy: 0.8571\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGRFMtkqHtZ7"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HhyRwRlHtaB"
   },
   "source": [
    "After training the model, the next step is **evaluation**. If you are satisfied with the model's performance based on the validation results, you can proceed to testing and finalizing your model. However, if the performance is not as expected, it may be necessary to adjust some **hyperparameters** before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42QmMeAoHtaC",
    "outputId": "730e1ee0-f3dc-43d8-ffba-f1ebc91a6cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid - Loss: 4.6340 Accuracy: 0.8571\n"
     ]
    }
   ],
   "source": [
    "utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
